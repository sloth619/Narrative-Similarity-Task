import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses, InputExample, models
from torch.utils.data import DataLoader
from datasets import load_dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator

import torch
from transformers import BitsAndBytesConfig
from peft import LoraConfig, TaskType, prepare_model_for_kbit_training


def main():
    print("ğŸš€ å¼€å§‹ Track B [ä¼˜åŒ–ç‰ˆ] è®­ç»ƒ (QLoRA)...")

    model_name = '/root/autodl-tmp/Qwen3-Embedding-4B'

    # === æ„å»ºæ¨¡å‹ ===
    print(f"Manually building model from: {model_name} with QLoRA")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    word_embedding_model = models.Transformer(
        model_name,
        tokenizer_args={'padding_side': 'left'},
        model_args={
            "quantization_config": bnb_config,
            "device_map": "auto",
        }
    )

    word_embedding_model.auto_model = prepare_model_for_kbit_training(
        word_embedding_model.auto_model,
        use_gradient_checkpointing=True
    )

    embedding_dim = word_embedding_model.get_word_embedding_dimension()
    print(f"Word Embedding Dimension: {embedding_dim}")

    pooling_model = models.Pooling(
        word_embedding_dimension=embedding_dim,
        pooling_mode='lasttoken'
    )
    model = SentenceTransformer(
        modules=[word_embedding_model, pooling_model],
        device='cuda'
    )

    # âœ… å¢åŠ  LoRA çš„è®­ç»ƒå®¹é‡
    lora_config = LoraConfig(
        r=32,  # 16 -> 32ï¼Œå¢åŠ ç§©
        lora_alpha=64,  # 32 -> 64
        lora_dropout=0.1,  # 0.05 -> 0.1ï¼Œå¢åŠ æ­£åˆ™åŒ–
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # å…¨éƒ¨å±‚
    )

    model.add_adapter(lora_config)

    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"âœ… Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}%")

    print("Model build successful with QLoRA.")

    # === åŠ è½½æ•°æ® ===
    print("æ­£åœ¨åŠ è½½: synthetic_data_for_contrastive_learning.jsonl")
    triplet_dataset = load_dataset('json',
                                   data_files='../../TrainingSet1/synthetic_data_for_contrastive_learning.jsonl',
                                   split='train')

    triplet_examples = []
    for item in triplet_dataset:
        anchor = item.get('anchor_story')
        positive = item.get('similar_story')
        negative = item.get('dissimilar_story')
        if all([anchor, positive, negative]):
            triplet_examples.append(InputExample(texts=[anchor, positive, negative]))
    print(f"åŠ è½½äº† {len(triplet_examples)} ä¸ªå¹²å‡€çš„ä¸‰å…ƒç»„æ ·æœ¬")

    print("æ­£åœ¨åŠ è½½: dev_track_b.jsonl")
    dev_b = load_dataset('json', data_files='../../TrainingSet1/dev_track_b.jsonl', split='train')

    pair_examples = []
    for item in dev_b:
        text = item.get('text')
        if text:
            pair_examples.append(InputExample(texts=[text, text]))
    print(f"åŠ è½½äº† {len(pair_examples)} ä¸ªå¹²å‡€çš„æ­£æ ·æœ¬å¯¹")

    triplet_loader = DataLoader(triplet_examples, shuffle=True, batch_size=64)
    triplet_loss = losses.TripletLoss(model=model, distance_metric=losses.TripletDistanceMetric.COSINE)

    pair_loader = DataLoader(pair_examples, shuffle=True, batch_size=64)
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === ä¿®æ”¹è¯„ä¼°å™¨ï¼šç§»é™¤ä¿å­˜åŠŸèƒ½ ===
    from train_b_evaluator_fixed import TrackB_Accuracy_Evaluator_NoSave
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="optimized",
        data_path="../../TrainingSet1/dev_track_a.jsonl"
    )

    # === è®­ç»ƒé…ç½® ===
    epochs = 5  # 2 -> 5ï¼Œå¢åŠ è®­ç»ƒè½®æ•°
    warmup_steps = 200  # 100 -> 200
    output_path = '../../output/track_b_optimized_model_qlora'
    os.makedirs(output_path, exist_ok=True)

    print(f"å¼€å§‹è®­ç»ƒï¼Œæ‰¹æ¬¡å¤§å°: triplet=64, pair=64, epochs=5")

    model.fit(
        train_objectives=[
            (triplet_loader, triplet_loss),  # è°ƒæ¢é¡ºåºï¼Œå…ˆ triplet
            (pair_loader, mnrl_loss),
        ],
        evaluator=evaluator,
        evaluation_steps=200,  # 500 -> 200ï¼Œæ›´é¢‘ç¹è¯„ä¼°
        epochs=epochs,
        warmup_steps=warmup_steps,
        output_path=output_path,
        save_best_model=False,  # ç¦ç”¨è‡ªåŠ¨ä¿å­˜
        show_progress_bar=True,
    )

    # æ‰‹åŠ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    try:
        model.save(output_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    except:
        model[0].auto_model.save_pretrained(os.path.join(output_path, "lora_adapter"))
        print(f"âœ… LoRA é€‚é…å™¨å·²ä¿å­˜")

    print("âœ… è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()