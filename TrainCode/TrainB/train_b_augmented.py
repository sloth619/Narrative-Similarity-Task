import os

# [FIX] è§£å†³æ˜¾å­˜ç¢ç‰‡
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses, InputExample, models  # å¯¼å…¥ 'models'
from torch.utils.data import DataLoader
from datasets import load_dataset
# ç¡®ä¿ä½ å·²ç»åˆ›å»ºäº† train_b_evaluator_fixed.py
from train_b_evaluator_fixed import TrackB_Accuracy_Evaluator_NoSave

# [FIX] å¯¼å…¥ QLoRA å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æ‰€éœ€çš„åº“
import torch
from transformers import BitsAndBytesConfig
from peft import LoraConfig, TaskType, prepare_model_for_kbit_training  # å¯¼å…¥ prepare_model_...


def main():
    print("ğŸš€ å¼€å§‹ Track B [AUGMENTED] è®­ç»ƒ (QLoRA ä¼˜åŒ–ç‰ˆ)...")

    model_name = '/root/autodl-tmp/Qwen3-Embedding-4B'

    # === æ„å»ºæ¨¡å‹ (ä¸ baseline å®Œå…¨ä¸€è‡´) ===
    print(f"Manually building model from: {model_name} with QLoRA")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    word_embedding_model = models.Transformer(
        model_name,
        tokenizer_args={'padding_side': 'left'},
        model_args={
            "quantization_config": bnb_config,
            "device_map": "auto",
        }
    )

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    word_embedding_model.auto_model = prepare_model_for_kbit_training(
        word_embedding_model.auto_model,
        use_gradient_checkpointing=True
    )

    embedding_dim = word_embedding_model.get_word_embedding_dimension()
    print(f"Word Embedding Dimension: {embedding_dim}")

    pooling_model = models.Pooling(
        word_embedding_dimension=embedding_dim,
        pooling_mode='lasttoken'
    )
    model = SentenceTransformer(
        modules=[word_embedding_model, pooling_model],
        device='cuda'
    )

    # å¢åŠ  LoRA çš„è®­ç»ƒå®¹é‡ (ä¸ baseline ä¸€è‡´)
    lora_config = LoraConfig(
        r=32,  # 16 -> 32
        lora_alpha=64,  # 32 -> 64
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # å…¨éƒ¨å±‚
    )

    model.add_adapter(lora_config)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"âœ… Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}%")

    if trainable_params == 0:
        raise RuntimeError("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼")

    print("Model build successful with QLoRA.")

    # === 1. åŠ è½½å¢å¼ºçš„è®­ç»ƒæ•°æ® (å·²ä¿®å¤è„æ•°æ®è¿‡æ»¤) ===

    print("æ­£åœ¨åŠ è½½: synthetic_data_for_contrastive_learning.jsonl (Triplets)")
    triplet_dataset = load_dataset('json',
                                   data_files='../../TrainingSet1/synthetic_data_for_contrastive_learning.jsonl',
                                   split='train')

    triplet_examples = []
    for item in triplet_dataset:
        anchor = item.get('anchor_story')
        positive = item.get('similar_story')
        negative = item.get('dissimilar_story')
        if all([anchor, positive, negative]):
            triplet_examples.append(InputExample(texts=[anchor, positive, negative]))
    print(f"åŠ è½½äº† {len(triplet_examples)} ä¸ªå¹²å‡€çš„ä¸‰å…ƒç»„æ ·æœ¬")

    print("æ­£åœ¨åŠ è½½: train_track_b_mixed_10k.jsonl (Augmented Pairs)")
    paired_dataset = load_dataset('json', data_files='../../TrainingSet2/train_track_b_mixed_10k.jsonl', split='train')

    # --- é…å¯¹ (Anchor, Positive) ---
    all_originals_map = {}
    print("æ­£åœ¨åŠ è½½: dev_track_b.jsonl (ç”¨äºåŒ¹é…é”šç‚¹)...")
    dev_b = load_dataset('json', data_files='../../TrainingSet1/dev_track_b.jsonl', split='train')
    for i, item in enumerate(dev_b):
        text = item.get('text')
        if text:
            all_originals_map[i] = text

    pair_examples = []
    for item in paired_dataset:
        if item.get('_augmented'):  # åªä½¿ç”¨æ–°ç”Ÿæˆçš„
            source_idx = item.get('_source_index')
            # ç¡®è®¤ V3.4 è„šæœ¬åªä» dev_b (ç´¢å¼• < 479) ç”Ÿæˆäº†æœ‰æ•ˆæ•°æ®
            if source_idx in all_originals_map:
                anchor_text = all_originals_map[source_idx]
                positive_text = item.get('text')

                if anchor_text and positive_text:
                    pair_examples.append(InputExample(
                        texts=[anchor_text, positive_text]
                    ))
    print(f"åŠ è½½äº† {len(pair_examples)} ä¸ªå¹²å‡€çš„å¢å¼ºæ­£æ ·æœ¬å¯¹")

    # === 2. å®šä¹‰æŸå¤±å‡½æ•° ===

    # ä½¿ç”¨ä¸ baseline ä¸€è‡´çš„æ‰¹å¤„ç†å¤§å°
    triplet_loader = DataLoader(triplet_examples, shuffle=True, batch_size=64)
    triplet_loss = losses.TripletLoss(model=model, distance_metric=losses.TripletDistanceMetric.COSINE)

    pair_loader = DataLoader(pair_examples, shuffle=True, batch_size=64)
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === 3. å®šä¹‰è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="augmented",
        data_path="../../TrainingSet1/dev_track_a.jsonl"
    )

    # === 4. å¼€å§‹è®­ç»ƒ ===
    epochs = 5  # å¢åŠ è®­ç»ƒè½®æ•°
    warmup_steps = 200  # å¢åŠ é¢„çƒ­
    output_path = '../../output/track_b_augmented_model_qlora'  # æ–°çš„è¾“å‡ºè·¯å¾„
    os.makedirs(output_path, exist_ok=True)

    print(f"å¼€å§‹è®­ç»ƒï¼Œæ‰¹æ¬¡å¤§å°: triplet=64, pair=64, epochs=5")

    model.fit(
        train_objectives=[
            (triplet_loader, triplet_loss),  # è°ƒæ¢é¡ºåº
            (pair_loader, mnrl_loss),
        ],
        evaluator=evaluator,
        evaluation_steps=200,  # æ›´é¢‘ç¹è¯„ä¼°
        epochs=epochs,
        warmup_steps=warmup_steps,
        output_path=output_path,
        save_best_model=False,  # ç¦ç”¨è‡ªåŠ¨ä¿å­˜
        show_progress_bar=True,
        # ä¸å†éœ€è¦æ¢¯åº¦ç´¯è®¡ï¼Œå› ä¸º batch_size å·²ç»å¾ˆå¤§
    )

    # æ‰‹åŠ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    try:
        model.save(output_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    except:
        model[0].auto_model.save_pretrained(os.path.join(output_path, "lora_adapter"))
        print(f"âœ… LoRA é€‚é…å™¨å·²ä¿å­˜")

    print("âœ… è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()