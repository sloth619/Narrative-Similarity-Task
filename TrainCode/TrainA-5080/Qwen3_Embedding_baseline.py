"""
Track Aé¢„æµ‹ - ä½¿ç”¨Track Bè®­ç»ƒå¥½çš„Embeddingæ¨¡å‹
"""
import json
import zipfile
from sentence_transformers import SentenceTransformer, models, util
from datasets import load_dataset
import torch
from transformers import BitsAndBytesConfig
from tqdm import tqdm

# é…ç½®
BASE_MODEL_PATH = '/mnt/e/model/Qwen3-Embedding-4B'
ADAPTER_PATH = '/mnt/e/Code/python/Narrative-Similarity-Task/output/track_b_from_synthetic_5080/checkpoint-356'
INPUT_DATA_FILE = '/mnt/e/Code/python/Narrative-Similarity-Task/TrainingSet1/dev_track_a.jsonl'
OUTPUT_DIR = '/mnt/e/Code/python/Narrative-Similarity-Task/submissions/track_a_from_embedding'


def main():
    print("ğŸš€ ä½¿ç”¨Track Bçš„Embeddingæ¨¡å‹åšTrack Aé¢„æµ‹")

    # åŠ è½½Track Bè®­ç»ƒå¥½çš„æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    word_embedding_model = models.Transformer(
        BASE_MODEL_PATH,
        tokenizer_args={'padding_side': 'left'},
        model_args={
            "quantization_config": bnb_config,
            "device_map": "auto",
        }
    )

    embedding_dim = word_embedding_model.get_word_embedding_dimension()
    pooling_model = models.Pooling(
        word_embedding_dimension=embedding_dim,
        pooling_mode='lasttoken'
    )

    model = SentenceTransformer(
        modules=[word_embedding_model, pooling_model],
        device='cuda'
    )

    model.load_adapter(ADAPTER_PATH)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½æ•°æ®
    dataset = load_dataset('json', data_files=INPUT_DATA_FILE, split='train')
    print(f"åŠ è½½äº† {len(dataset)} ä¸ªä¸‰å…ƒç»„")

    # é¢„æµ‹
    predictions = []
    for item in tqdm(dataset, desc="Predicting"):
        anchor = item['anchor_text']
        text_a = item['text_a']
        text_b = item['text_b']

        # ç¼–ç 
        embeddings = model.encode(
            [anchor, text_a, text_b],
            convert_to_tensor=True,
            show_progress_bar=False
        )

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim_a = util.cos_sim(embeddings[0], embeddings[1]).item()
        sim_b = util.cos_sim(embeddings[0], embeddings[2]).item()

        # é¢„æµ‹
        pred = sim_a > sim_b

        predictions.append({
            'anchor_text': anchor,
            'text_a': text_a,
            'text_b': text_b,
            'text_a_is_closer': bool(pred)
        })

    # ä¿å­˜
    import os
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    output_jsonl = os.path.join(OUTPUT_DIR, 'predictions.jsonl')
    with open(output_jsonl, 'w', encoding='utf-8') as f:
        for pred in predictions:
            f.write(json.dumps(pred, ensure_ascii=False) + '\n')

    # æ‰“åŒ…
    output_zip = os.path.join(OUTPUT_DIR, 'submission.zip')
    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zf:
        zf.write(output_jsonl, arcname='track_a.jsonl')

    print(f"âœ… å®Œæˆ! æäº¤æ–‡ä»¶: {output_zip}")

    # ç»Ÿè®¡
    true_count = sum(1 for p in predictions if p['text_a_is_closer'])
    print(f"\né¢„æµ‹åˆ†å¸ƒ:")
    print(f"  text_aæ›´æ¥è¿‘: {true_count} ({true_count / len(predictions) * 100:.1f}%)")
    print(
        f"  text_bæ›´æ¥è¿‘: {len(predictions) - true_count} ({(len(predictions) - true_count) / len(predictions) * 100:.1f}%)")


if __name__ == "__main__":
    main()