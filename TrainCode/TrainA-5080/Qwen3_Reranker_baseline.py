"""
Track Aè®­ç»ƒ - Qwen3-Reranker-4B with QLoRA (WSL on 5080)
ä½¿ç”¨Syntheticæ•°æ® + QLoRAå¾®è°ƒ - ä¼˜åŒ–æ˜¾å­˜ç‰ˆæœ¬
"""
import os

# è§£å†³tokenizersè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import Trainer, TrainingArguments
from transformers import BitsAndBytesConfig
from transformers.trainer_callback import TrainerCallback
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset, load_dataset
from tqdm import tqdm
import json


def build_pairs_from_track_a(data_path):
    """ä»Track Aæ„å»ºè®­ç»ƒæ•°æ® - ç”Ÿæˆæ­£è´Ÿæ ·æœ¬å¯¹"""
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    for item in dataset:
        anchor = item.get('anchor_text')
        text_a = item.get('text_a')
        text_b = item.get('text_b')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            continue

        # ç¡®å®šæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
            negative = text_b if label_a_closer else text_a
        else:
            positive = text_a
            negative = text_b

        # æ·»åŠ æ­£æ ·æœ¬å¯¹ (label=1.0)
        train_data.append({
            'text1': anchor,
            'text2': positive,
            'label': 1.0
        })

        # æ·»åŠ è´Ÿæ ·æœ¬å¯¹ (label=0.0)
        train_data.append({
            'text1': anchor,
            'text2': negative,
            'label': 0.0
        })

    return Dataset.from_list(train_data)


class TrackA_Accuracy_Evaluator:
    """Track Aå‡†ç¡®ç‡è¯„ä¼°å™¨ - ä¸‰é€‰ä¸€åˆ†ç±»"""

    def __init__(self, name: str, data_path: str, tokenizer, max_length: int = 512):
        self.name = name
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []
        self.load_data(data_path)

    def load_data(self, data_path: str):
        """åŠ è½½éªŒè¯æ•°æ®"""
        print(f"Evaluator: æ­£åœ¨åŠ è½½å¹¶æ¸…æ´— {data_path}...")

        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    self.samples.append({
                        'anchor': data['anchor_text'],
                        'text_a': data['text_a'],
                        'text_b': data['text_b'],
                        'label': data['text_a_is_closer']
                    })
                except:
                    continue

        print(f"Evaluator: åŠ è½½äº† {len(self.samples)} ä¸ªå¹²å‡€çš„éªŒè¯æ ·æœ¬ã€‚\n")

    def __call__(self, model, device):
        """è¯„ä¼°æ¨¡å‹å¹¶è¿”å›å‡†ç¡®ç‡"""
        model.eval()
        correct = 0
        total = len(self.samples)

        with torch.no_grad():
            for sample in self.samples:
                # ç¼–ç  anchor-text_a
                inputs_a = self.tokenizer(
                    sample['anchor'],
                    sample['text_a'],
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors='pt'
                ).to(device)

                # ç¼–ç  anchor-text_b
                inputs_b = self.tokenizer(
                    sample['anchor'],
                    sample['text_b'],
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors='pt'
                ).to(device)

                # è·å–ç›¸å…³æ€§åˆ†æ•°
                score_a = model(**inputs_a).logits.squeeze().item()
                score_b = model(**inputs_b).logits.squeeze().item()

                # é¢„æµ‹: text_aåˆ†æ•°æ›´é«˜åˆ™ä¸ºTrue
                pred = score_a > score_b

                if pred == sample['label']:
                    correct += 1

        accuracy = correct / total
        model.train()
        return accuracy


class EvaluateCallback(TrainerCallback):
    """è‡ªå®šä¹‰å›è°ƒ - åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¯„ä¼°"""

    def __init__(self, evaluator, device):
        self.evaluator = evaluator
        self.device = device
        self.best_accuracy = 0.0

    def on_epoch_end(self, args, state, control, model=None, **kwargs):
        """Epochç»“æŸæ—¶è¯„ä¼°"""
        if model is not None:
            print(f"\n[Validation {self.evaluator.name}] Epoch: {state.epoch:.1f}, Steps: {state.global_step}")

            accuracy = self.evaluator(model, self.device)

            # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³
            is_best = accuracy > self.best_accuracy
            if is_best:
                self.best_accuracy = accuracy

            print(f"Accuracy: {accuracy:.4f} ({int(accuracy*len(self.evaluator.samples))}/{len(self.evaluator.samples)}){' â­ New best!' if is_best else ''}")

        return control


def main():
    print("ğŸš€ Track Aè®­ç»ƒ - Qwen3-Reranker-4B with QLoRA (WSL on 5080)...")
    print("ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ç‰ˆæœ¬")

    # === WSLè·¯å¾„é…ç½® ===
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    model_name = '/mnt/e/model/Qwen3-Reranker-4B'
    output_path = f'{PROJECT_ROOT}/output/track_a_qwen3_reranker_4B_qlora_wsl'
    os.makedirs(output_path, exist_ok=True)

    dev_track_a_path = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'
    synthetic_data_path = f'{PROJECT_ROOT}/TrainingSet1/synthetic_data_for_classification.jsonl'

    # === æ„å»ºæ¨¡å‹ with QLoRA ===
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    print("ä½¿ç”¨4-bité‡åŒ–é…ç½®...")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    # åŠ è½½Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # åŠ è½½æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=1,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )

    # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    model = prepare_model_for_kbit_training(
        model,
        use_gradient_checkpointing=True
    )

    print(f"æ¨¡å‹åŠ è½½å®Œæˆ")

    # === LoRAé…ç½® ===
    print("\né…ç½®LoRAé€‚é…å™¨...")
    lora_config = LoraConfig(
        r=32,
        lora_alpha=64,
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.SEQ_CLS,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )

    model = get_peft_model(model, lora_config)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_param = sum(p.numel() for p in model.parameters())
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {all_param:,} ({100 * trainable_params / all_param:.2f}%)")

    # === åŠ è½½æ•°æ® ===
    print("\nä»syntheticæ•°æ®æ„å»ºè®­ç»ƒé›†...")
    train_dataset = build_pairs_from_track_a(synthetic_data_path)
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    # === æ•°æ®é¢„å¤„ç† ===
    def preprocess_function(examples):
        return tokenizer(
            examples['text1'],
            examples['text2'],
            padding='max_length',
            truncation=True,
            max_length=512,
        )

    train_dataset = train_dataset.map(
        preprocess_function,
        batched=True,
        desc="Tokenizing"
    )

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackA_Accuracy_Evaluator(
        name="reranker_4B_synthetic",
        data_path=dev_track_a_path,
        tokenizer=tokenizer,
        max_length=512
    )

    # === è®­ç»ƒé…ç½® ===
    epochs = 3

    training_args = TrainingArguments(
        output_dir=output_path,
        num_train_epochs=epochs,
        per_device_train_batch_size=16,
        gradient_accumulation_steps=2,
        learning_rate=2e-5,
        warmup_ratio=0.1,
        eval_strategy="no",  # æ”¹ä¸ºno,ä½¿ç”¨callbackæ‰‹åŠ¨è¯„ä¼°
        save_strategy="epoch",
        save_total_limit=2,
        logging_steps=50,
        bf16=True,
        optim="adamw_8bit",
        gradient_checkpointing=True,
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        max_grad_norm=0.3,
        remove_unused_columns=False,
        label_names=["labels"],
    )

    print(f"\nå¼€å§‹è®­ç»ƒ:")
    print(f"  - æ¨¡å‹: Qwen3-Reranker-4B with QLoRA")
    print(f"  - Batch size: {training_args.per_device_train_batch_size}")
    print(f"  - Gradient Accumulation: {training_args.gradient_accumulation_steps}")
    print(f"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"  - Learning rate: {training_args.learning_rate}")
    print(f"  - Epochs: {epochs}")
    print(f"  - LoRA r: {lora_config.r}")
    print(f"  - Gradient Checkpointing: âœ…")

    # === æ•°æ®æ•´ç†å‡½æ•° ===
    def data_collator(features):
        batch = {
            'input_ids': torch.stack([torch.tensor(f['input_ids']) for f in features]),
            'attention_mask': torch.stack([torch.tensor(f['attention_mask']) for f in features]),
            'labels': torch.tensor([f['label'] for f in features], dtype=torch.float32),
        }
        return batch

    # === æ¸…ç†æ˜¾å­˜ ===
    torch.cuda.empty_cache()

    # === åˆ›å»ºè¯„ä¼°å›è°ƒ ===
    eval_callback = EvaluateCallback(
        evaluator=evaluator,
        device=training_args.device
    )

    # === è®­ç»ƒ ===
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        callbacks=[eval_callback],
    )

    print("\nå¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # === æœ€åè¯„ä¼°ä¸€æ¬¡ ===
    print(f"\næœ€ç»ˆè¯„ä¼°...")
    final_accuracy = evaluator(model, training_args.device)
    print(f"Final Accuracy: {final_accuracy:.4f}")

    # === ä¿å­˜ ===
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    try:
        trainer.save_model(output_path)
        tokenizer.save_pretrained(output_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"å®Œæ•´æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        lora_adapter_path = os.path.join(output_path, "lora_adapter")
        model.save_pretrained(lora_adapter_path)
        tokenizer.save_pretrained(lora_adapter_path)
        print(f"âœ… LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {lora_adapter_path}")

    print("\nâœ… è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()