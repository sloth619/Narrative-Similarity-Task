"""
Track Aé¢„æµ‹ - ä½¿ç”¨Qwen3-Instruct-4B (ä¿®å¤è§£æé€»è¾‘)
"""
import os
import json
import zipfile
from datasets import load_dataset
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# --- é…ç½® ---
MODEL_PATH = '/mnt/e/model/Qwen3-4B-Instruct-2507'
INPUT_DATA_FILE = '/mnt/e/Code/python/Narrative-Similarity-Task/TrainingSet1/dev_track_a.jsonl'
OUTPUT_DIR = '/mnt/e/Code/python/Narrative-Similarity-Task/submissions/track_a_qwen_instruct_submission'
OUTPUT_JSONL_FILE = 'track_a.jsonl'
OUTPUT_ZIP_FILE = 'submission.zip'


def load_qwen_instruct_model(model_path):
    """åŠ è½½Qwen3-Instructæ¨¡å‹ (4bité‡åŒ–)"""
    print("ğŸ”§ åŠ è½½ Qwen3-Instruct-4B æ¨¡å‹...")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        local_files_only=True
    )

    print(f"âœ… Qwen3-Instruct åŠ è½½æˆåŠŸ")
    return tokenizer, model


def create_prompt(anchor, text_a, text_b):
    """åˆ›å»ºç®€æ´çš„prompt,å¼ºåˆ¶åªè¾“å‡ºAæˆ–B"""
    prompt = f"""You are an expert in narrative analysis. Determine which story is more similar to the anchor story in terms of themes, plot structure, and outcomes.

Anchor Story:
{anchor}

Story A:
{text_a}

Story B:
{text_b}

Question: Which story (A or B) is more similar to the Anchor?
Important: Answer with ONLY the letter A or B, nothing else.

Answer:"""

    return prompt


def predict_with_instruct(tokenizer, model, anchor, text_a, text_b):
    """
    ä½¿ç”¨Instructæ¨¡å‹è¿›è¡Œé¢„æµ‹
    ä¿®å¤è§£æé€»è¾‘: åªçœ‹ç¬¬ä¸€ä¸ªå­—ç¬¦
    """
    prompt = create_prompt(anchor, text_a, text_b)

    messages = [
        {"role": "system", "content": "You are a helpful assistant. Answer only with A or B."},
        {"role": "user", "content": prompt}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=2048
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,  # å…è®¸æ›´å¤štokensæ¥ç”Ÿæˆå®Œæ•´å›ç­”
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:],
        skip_special_tokens=True
    ).strip()

    # ğŸ”§ ä¿®å¤çš„è§£æé€»è¾‘
    # 1. é¦–å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªAæˆ–B
    first_char = None
    for char in response:
        if char.upper() == 'A':
            first_char = 'A'
            break
        elif char.upper() == 'B':
            first_char = 'B'
            break

    if first_char == 'A':
        return True, response  # text_aæ›´æ¥è¿‘
    elif first_char == 'B':
        return False, response  # text_bæ›´æ¥è¿‘
    else:
        # å®Œå…¨æ— æ³•è§£æ,éšæœºé»˜è®¤
        print(f"âš ï¸  å®Œå…¨æ— æ³•è§£æ: '{response[:50]}...', é»˜è®¤é€‰A")
        return True, response


def main():
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ Qwen3-Instruct Track A æäº¤æ–‡ä»¶...")
    print(f"   æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"   è¾“å…¥æ•°æ®: {INPUT_DATA_FILE}")

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # === 1. åŠ è½½æ¨¡å‹ ===
    tokenizer, model = load_qwen_instruct_model(MODEL_PATH)

    # === 2. åŠ è½½è€ƒé¢˜æ•°æ® ===
    print(f"\næ­£åœ¨åŠ è½½è€ƒé¢˜: {INPUT_DATA_FILE}")
    dataset = load_dataset('json', data_files=INPUT_DATA_FILE, split='train')
    print(f"å·²åŠ è½½ {len(dataset)} ä¸ªä¸‰å…ƒç»„\n")

    # === 3. æ‰¹é‡é¢„æµ‹ ===
    print("å¼€å§‹é¢„æµ‹ (ä¿®å¤äº†è§£æé€»è¾‘)...\n")

    predictions = []
    parse_errors = 0
    response_stats = {'A': 0, 'B': 0, 'error': 0}

    # ä¿å­˜ä¸€äº›æ ·ä¾‹ç”¨äºåˆ†æ
    sample_responses = []

    for idx, item in enumerate(tqdm(dataset, desc="Predicting with Instruct"), 1):
        anchor = item.get('anchor_text', '')
        text_a = item.get('text_a', '')
        text_b = item.get('text_b', '')

        if not all([anchor, text_a, text_b]):
            predictions.append({
                'anchor_text': anchor,
                'text_a': text_a,
                'text_b': text_b,
                'text_a_is_closer': True
            })
            continue

        try:
            pred, response = predict_with_instruct(tokenizer, model, anchor, text_a, text_b)

            # ç»Ÿè®¡
            if pred:
                response_stats['A'] += 1
            else:
                response_stats['B'] += 1

            # ä¿å­˜å‰5ä¸ªæ ·ä¾‹
            if len(sample_responses) < 5:
                sample_responses.append({
                    'idx': idx,
                    'pred': 'A' if pred else 'B',
                    'response': response[:100]
                })

        except Exception as e:
            print(f"âš ï¸  æ ·æœ¬ {idx} é¢„æµ‹å‡ºé”™: {e}, é»˜è®¤é€‰A")
            pred = True
            parse_errors += 1
            response_stats['error'] += 1

        predictions.append({
            'anchor_text': anchor,
            'text_a': text_a,
            'text_b': text_b,
            'text_a_is_closer': bool(pred)
        })

    print(f"\nâœ… é¢„æµ‹å®Œæˆï¼Œå…± {len(predictions)} ä¸ªæ ·æœ¬")
    print(f"\nğŸ“Š æ¨¡å‹å›ç­”ç»Ÿè®¡:")
    print(f"   é€‰æ‹©A: {response_stats['A']} ({response_stats['A']/len(predictions)*100:.1f}%)")
    print(f"   é€‰æ‹©B: {response_stats['B']} ({response_stats['B']/len(predictions)*100:.1f}%)")
    print(f"   è§£æé”™è¯¯: {response_stats['error']} ({response_stats['error']/len(predictions)*100:.1f}%)")

    # æ˜¾ç¤ºæ ·ä¾‹å“åº”
    print(f"\nğŸ” æ¨¡å‹å›ç­”æ ·ä¾‹:")
    for sample in sample_responses:
        print(f"\næ ·æœ¬ {sample['idx']}: é€‰æ‹© {sample['pred']}")
        print(f"  å›ç­”: {sample['response']}...")

    # === 4. å†™å…¥æ–‡ä»¶ ===
    output_jsonl_path = os.path.join(OUTPUT_DIR, OUTPUT_JSONL_FILE)
    print(f"\næ­£åœ¨å†™å…¥ {output_jsonl_path} ...")

    with open(output_jsonl_path, 'w', encoding='utf-8') as f:
        for pred in predictions:
            f.write(json.dumps(pred, ensure_ascii=False) + '\n')

    print(f"âœ… {OUTPUT_JSONL_FILE} å†™å…¥æˆåŠŸ")

    # === 5. æ‰“åŒ… ===
    output_zip_path = os.path.join(OUTPUT_DIR, OUTPUT_ZIP_FILE)
    print(f"\næ­£åœ¨åˆ›å»º {output_zip_path} ...")

    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        zf.write(output_jsonl_path, arcname=OUTPUT_JSONL_FILE)

    print(f"\nğŸ‰ æäº¤æ–‡ä»¶å·²ç”Ÿæˆï¼")
    print(f"ğŸ“ è¾“å‡ºä½ç½®: {output_zip_path}")

    # === 6. æœ€ç»ˆç»Ÿè®¡ ===
    true_count = sum(1 for p in predictions if p['text_a_is_closer'])
    false_count = len(predictions) - true_count

    print(f"\nğŸ“Š æœ€ç»ˆé¢„æµ‹åˆ†å¸ƒ:")
    print(f"   text_aæ›´æ¥è¿‘: {true_count} ({true_count / len(predictions) * 100:.1f}%)")
    print(f"   text_bæ›´æ¥è¿‘: {false_count} ({false_count / len(predictions) * 100:.1f}%)")


if __name__ == "__main__":
    main()