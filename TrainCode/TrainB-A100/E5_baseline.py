"""
Track Bè®­ç»ƒ - E5-large-v2 (A100 40GBä¼˜åŒ–ç‰ˆ)
åªç”¨Syntheticé«˜è´¨é‡æ•°æ®
"""
import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses
from datasets import load_dataset, Dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave

from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

import torch


def build_triplets_from_track_a(data_path, add_prefix=True):
    """ä»Track Aæ„å»ºè®­ç»ƒæ•°æ® (å¸¦è¯¦ç»†è°ƒè¯•)"""
    print(f"  æ­£åœ¨åŠ è½½: {data_path}")

    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(data_path):
        print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return Dataset.from_list([])

    dataset = load_dataset('json', data_files=data_path, split='train')
    print(f"  åŸå§‹æ•°æ®è¡Œæ•°: {len(dataset)}")

    # çœ‹ç¬¬ä¸€æ¡æ•°æ®çš„keys
    if len(dataset) > 0:
        print(f"  æ•°æ®å­—æ®µ: {list(dataset[0].keys())}")

    train_data = []
    skipped = 0

    for item in dataset:
        # å°è¯•å¤šç§å­—æ®µå
        anchor = item.get('anchor_text') or item.get('anchor_story') or item.get('anchor')
        text_a = item.get('text_a') or item.get('similar_story') or item.get('positive')
        text_b = item.get('text_b') or item.get('dissimilar_story') or item.get('negative')
        label_a_closer = item.get('text_a_is_closer')

        # å¦‚æœä¸‰ä¸ªéƒ½æ²¡æœ‰,è·³è¿‡
        if not all([anchor, text_a, text_b]):
            skipped += 1
            continue

        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
        else:
            positive = text_a

        # E5éœ€è¦åŠ å‰ç¼€
        if add_prefix:
            anchor = f"query: {anchor}"
            positive = f"passage: {positive}"

        train_data.append({'sentence1': anchor, 'sentence2': positive})
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})

    if skipped > 0:
        print(f"  âš ï¸ è·³è¿‡äº† {skipped} æ¡æ•°æ® (ç¼ºå°‘å­—æ®µ)")

    print(f"  âœ… ç”Ÿæˆäº† {len(train_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
    return Dataset.from_list(train_data)


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - E5-large-v2 (A100 40GBä¼˜åŒ–ç‰ˆ)...")

    # === A100è·¯å¾„é…ç½® ===
    model_path = '/root/autodl-tmp/Narrative-Similarity-Task/models/e5-large-v2'
    output_path = '/root/autodl-tmp/Narrative-Similarity-Task/output/track_b_e5_a100'
    os.makedirs(output_path, exist_ok=True)

    # === æ£€æŸ¥æ–­ç‚¹ ===
    checkpoint_path = None
    if os.path.exists(output_path):
        checkpoints = [d for d in os.listdir(output_path) if d.startswith('checkpoint-')]
        if checkpoints:
            checkpoints.sort(key=lambda x: int(x.split('-')[1]))
            checkpoint_path = os.path.join(output_path, checkpoints[-1])
            print(f"âœ… æ‰¾åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")

    # === åŠ è½½æ¨¡å‹ ===
    if checkpoint_path:
        print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹...")
        model = SentenceTransformer(checkpoint_path)
        print("âœ… æ¨¡å‹ä»æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ")
    else:
        print("åŠ è½½æ¨¡å‹: intfloat/e5-large-v2")
        try:
            model = SentenceTransformer(model_path)
            print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"æœ¬åœ°åŠ è½½å¤±è´¥: {e}")
            print("ä»HuggingFaceä¸‹è½½...")
            model = SentenceTransformer('intfloat/e5-large-v2')
            print("âœ… HFæ¨¡å‹åŠ è½½æˆåŠŸ")

        print(f"   Embeddingç»´åº¦: {model.get_sentence_embedding_dimension()}")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # === åŠ è½½æ•°æ® ===
    print("\nåŠ è½½è®­ç»ƒæ•°æ®...")

    print("1. åŠ è½½Syntheticæ•°æ®...")
    train_dataset = build_triplets_from_track_a(
        '/root/autodl-tmp/Narrative-Similarity-Task/TrainingSet1/synthetic_data_for_contrastive_learning.jsonl',
        add_prefix=True  # E5éœ€è¦å‰ç¼€
    )

    print(f"\næ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    if len(train_dataset) == 0:
        print("âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®!è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼")
        return

    # === æŸå¤±å‡½æ•° ===
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="e5_a100",
        data_path="/root/autodl-tmp/Narrative-Similarity-Task/TrainingSet1/dev_track_a.jsonl",
        batch_size=64  # A100å¯ä»¥ç”¨æ›´å¤§çš„è¯„ä¼°batch
    )

    # === è®­ç»ƒé…ç½® (A100ä¼˜åŒ–) ===
    epochs = 5

    training_args = SentenceTransformerTrainingArguments(
        output_dir=output_path,
        num_train_epochs=epochs,
        per_device_train_batch_size=64,  # A100å¤§batch
        gradient_accumulation_steps=1,
        learning_rate=2e-5,  # E5æ¨è2e-5
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        logging_steps=20,
        metric_for_best_model="eval_evaluator",
        bf16=True,
        dataloader_num_workers=4,  # A100å¤šè¿›ç¨‹
        dataloader_pin_memory=True,
        resume_from_checkpoint=checkpoint_path,
    )

    print(f"\nå¼€å§‹è®­ç»ƒ:")
    print(f"  - ç¡¬ä»¶: A100 40GB")
    if checkpoint_path:
        print(f"  - âœ… æ–­ç‚¹ç»­ä¼ : {checkpoint_path}")
    else:
        print(f"  - ğŸ†• ä»å¤´è®­ç»ƒ")
    print(f"  - æ¨¡å‹: E5-large-v2")
    print(f"  - è®­ç»ƒæ•°æ®: Synthetic only (é«˜è´¨é‡)")
    print(f"  - E5å‰ç¼€: query: / passage:")
    print(f"  - æ€»æ ·æœ¬: {len(train_dataset):,}")
    print(f"  - Batch size: {training_args.per_device_train_batch_size}")
    print(f"  - Learning rate: {training_args.learning_rate}")
    print(f"  - Epochs: {epochs}")
    print(f"  - é¢„è®¡æ­¥æ•°: {len(train_dataset) // training_args.per_device_train_batch_size * epochs}")
    print(f"  - é¢„è®¡æ—¶é—´: ~6-8åˆ†é’Ÿ")

    # === è®­ç»ƒ ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    try:
        trainer.train(resume_from_checkpoint=checkpoint_path)
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­!")
        print("ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜,å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­è®­ç»ƒ")
        return

    # === ä¿å­˜ ===
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(output_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")

    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"\né¢„æœŸå‡†ç¡®ç‡: 64-66%")
    print(f"(E5 + Synthetic + A100å¤§batch)")


if __name__ == "__main__":
    main()