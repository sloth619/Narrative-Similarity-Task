import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
from sentence_transformers import SentenceTransformer, losses, InputExample, models
from torch.utils.data import DataLoader
from datasets import load_dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave
import torch
from transformers import BitsAndBytesConfig
from peft import LoraConfig, TaskType, prepare_model_for_kbit_training


def main():
    print("ğŸš€ å¼€å§‹ Track B [AUGMENTED V2] è®­ç»ƒ (QLoRA + ä½å­¦ä¹ ç‡ + 7 æ¨¡å—)...")

    model_name = '/root/autodl-tmp/Qwen3-Embedding-4B'

    # === æ„å»ºæ¨¡å‹ ===
    print(f"Manually building model from: {model_name} with QLoRA")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    word_embedding_model = models.Transformer(
        model_name,
        tokenizer_args={'padding_side': 'left'},
        model_args={
            "quantization_config": bnb_config,
            "device_map": "auto",
        }
    )

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    word_embedding_model.auto_model = prepare_model_for_kbit_training(
        word_embedding_model.auto_model,
        use_gradient_checkpointing=True
    )

    embedding_dim = word_embedding_model.get_word_embedding_dimension()
    print(f"Word Embedding Dimension: {embedding_dim}")

    pooling_model = models.Pooling(
        word_embedding_dimension=embedding_dim,
        pooling_mode='lasttoken'
    )
    model = SentenceTransformer(
        modules=[word_embedding_model, pooling_model],
        device='cuda'
    )

    lora_config = LoraConfig(
        r=32,
        lora_alpha=64,
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # å…¨éƒ¨ 7 ä¸ªå±‚
    )

    model.add_adapter(lora_config)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"âœ… Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}%")

    if trainable_params == 0:
        raise RuntimeError("âŒ æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼")

    print("Model build successful with QLoRA.")

    # === 1. åŠ è½½å¢å¼ºçš„è®­ç»ƒæ•°æ® (å·²ä¿®å¤ 1913 bug) ===

    print("æ­£åœ¨åŠ è½½: train_track_b_mixed_10k.jsonl (Augmented Pairs)")
    paired_dataset = load_dataset('json', data_files='../../TrainingSet2/train_track_b_mixed_10k.jsonl', split='train')

    # --- â— [FIX] ä¿®å¤é”šç‚¹åŒ¹é…é€»è¾‘ ---
    all_originals_map = {}
    print("æ­£åœ¨åŠ è½½ dev_b (ç”¨äºåŒ¹é…é”šç‚¹)...")
    dev_b = load_dataset('json', data_files='../../TrainingSet1/dev_track_b.jsonl', split='train')
    for i, item in enumerate(dev_b):
        text = item.get('text')
        if text:
            all_originals_map[i] = text  # ç´¢å¼• 0-478

    print("æ­£åœ¨åŠ è½½ synthetic_b (ç”¨äºåŒ¹é…é”šç‚¹)...")
    synthetic_b_offset = len(dev_b)  # 479
    synthetic_b = load_dataset('json', data_files='../../TrainingSet1/synthetic_data_for_contrastive_learning.jsonl',
                               split='train')
    for i, item in enumerate(synthetic_b):
        text = item.get('anchor_story') or item.get('text')  # ç¡®ä¿èƒ½è¯»åˆ°
        if text:
            all_originals_map[i + synthetic_b_offset] = text  # ç´¢å¼• 479-2375
    # --- End of Fix ---

    pair_examples = []
    for item in paired_dataset:
        if item.get('_augmented'):
            source_idx = item.get('_source_index')
            if source_idx in all_originals_map:
                anchor_text = all_originals_map[source_idx]
                positive_text = item.get('text')

                if anchor_text and positive_text:
                    pair_examples.append(InputExample(
                        texts=[anchor_text, positive_text]
                    ))
    print(f"åŠ è½½äº† {len(pair_examples)} ä¸ªå¹²å‡€çš„å¢å¼ºæ­£æ ·æœ¬å¯¹ (å·²ä¿®å¤)")

    # === 2. å®šä¹‰æŸå¤±å‡½æ•° ===

    # [FIX] åªä½¿ç”¨ MNRL æŸå¤±å‡½æ•°
    pair_loader = DataLoader(pair_examples, shuffle=True, batch_size=64)
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === 3. å®šä¹‰è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="augmented_low_lr",
        data_path="../../TrainingSet1/dev_track_a.jsonl",
        batch_size=64
    )

    # === 4. å¼€å§‹è®­ç»ƒ ===
    epochs = 3  # è®­ç»ƒ 3 è½®
    warmup_steps = int(len(pair_loader) * epochs * 0.1)  # 10% é¢„çƒ­
    output_path = '../../output/track_b_augmented_model_v2_qlora'
    os.makedirs(output_path, exist_ok=True)

    print(f"å¼€å§‹è®­ç»ƒï¼Œæ‰¹æ¬¡å¤§å°: pair=64, epochs=3")

    model.fit(
        train_objectives=[
            (pair_loader, mnrl_loss),
            # [FIX] ç§»é™¤æ—§çš„ triplet æŸå¤±
        ],
        evaluator=evaluator,
        evaluation_steps=200,
        epochs=epochs,
        warmup_steps=warmup_steps,
        output_path=output_path,
        save_best_model=False,
        show_progress_bar=True,
        learning_rate=5e-7  # â—â— [FIX] è®¾ç½®ä¸€ä¸ªéå¸¸ä½çš„å¾®è°ƒå­¦ä¹ ç‡ â—â—
    )

    # æ‰‹åŠ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    try:
        model.save(output_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"è­¦å‘Š: model.save() å¤±è´¥: {e}")
        print("å°è¯•ä»…ä¿å­˜ LoRA é€‚é…å™¨...")
        try:
            model[0].auto_model.save_pretrained(os.path.join(output_path, "lora_adapter"))
            print(f"âœ… LoRA é€‚é…å™¨å·²ä¿å­˜")
        except Exception as e2:
            print(f"âŒ é€‚é…å™¨ä¿å­˜ä¹Ÿå¤±è´¥äº†: {e2}")

    print("âœ… è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()