"""
ä¸‰æ¨¡å‹Ensemble - E5 + Gemini 2.5 Pro + Qwen3-Max
"""
import json
import time
import os
from typing import List, Dict, Optional
from tqdm import tqdm
from google import genai
from google.genai import types
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from datasets import load_dataset
import torch
import numpy as np


class TripleModelEnsemble:
    """E5 + Gemini 2.5 Pro + Qwen3-Max ä¸‰æ¨¡å‹é›†æˆ"""

    def __init__(
            self,
            e5_model_path: str,
            gemini_api_keys: List[str],
            qwen_api_keys: List[str],
            e5_weight: float = 0.3,
            gemini_weight: float = 0.4,
            qwen_weight: float = 0.3,
            use_gemini_thinking: bool = True
    ):
        """
        Args:
            e5_model_path: E5æ¨¡å‹è·¯å¾„
            gemini_api_keys: Gemini API keys
            qwen_api_keys: Qwen API keys
            e5_weight: E5æƒé‡
            gemini_weight: Geminiæƒé‡
            qwen_weight: Qwenæƒé‡
            use_gemini_thinking: Geminiæ˜¯å¦ä½¿ç”¨æ€è€ƒæ¨¡å¼
        """
        assert abs(e5_weight + gemini_weight + qwen_weight - 1.0) < 0.001, "æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1"

        self.e5_weight = e5_weight
        self.gemini_weight = gemini_weight
        self.qwen_weight = qwen_weight
        self.use_gemini_thinking = use_gemini_thinking

        # åŠ è½½E5
        print("ğŸ“¦ åŠ è½½E5æ¨¡å‹...")
        self.e5_model = SentenceTransformer(e5_model_path)
        print(f"   âœ… E5åŠ è½½æˆåŠŸ")

        # API keys
        self.gemini_keys = gemini_api_keys
        self.qwen_keys = qwen_api_keys
        self.gemini_key_index = 0
        self.qwen_key_index = 0

        print(f"   Gemini Keys: {len(gemini_api_keys)}ä¸ª")
        print(f"   Qwen Keys: {len(qwen_api_keys)}ä¸ª")

    def _predict_e5(self, anchor: str, text_a: str, text_b: str) -> float:
        """E5é¢„æµ‹ - è¿”å›0-1ç½®ä¿¡åº¦"""
        anchor_prefixed = f"query: {anchor}"
        text_a_prefixed = f"passage: {text_a}"
        text_b_prefixed = f"passage: {text_b}"

        embeddings = self.e5_model.encode(
            [anchor_prefixed, text_a_prefixed, text_b_prefixed],
            convert_to_tensor=True,
            show_progress_bar=False
        )

        sim_a = torch.nn.functional.cosine_similarity(
            embeddings[0].unsqueeze(0),
            embeddings[1].unsqueeze(0)
        ).item()

        sim_b = torch.nn.functional.cosine_similarity(
            embeddings[0].unsqueeze(0),
            embeddings[2].unsqueeze(0)
        ).item()

        diff = sim_a - sim_b
        confidence = 1 / (1 + np.exp(-10 * diff))

        return confidence

    def _predict_gemini(self, anchor: str, text_a: str, text_b: str) -> Optional[float]:
        """Gemini 2.5 Proé¢„æµ‹"""
        prompt = f"""You are an expert in narrative analysis. Compare three stories to determine narrative similarity.

NARRATIVE SIMILARITY:
1. **Abstract Theme** (30%): Core ideas, conflicts, motifs
2. **Course of Action** (40%): Event sequence, plot structure
3. **Outcomes** (30%): Final resolution, character fates

ANCHOR STORY:
{anchor}

CANDIDATE A:
{text_a}

CANDIDATE B:
{text_b}

Which candidate (A or B) is MORE narratively similar to the Anchor?
Respond with ONLY: A or B"""

        safety_settings = [
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE")
        ]

        for retry in range(2):
            try:
                api_key = self.gemini_keys[self.gemini_key_index]
                self.gemini_key_index = (self.gemini_key_index + 1) % len(self.gemini_keys)

                time.sleep(2)
                client = genai.Client(api_key=api_key)

                config = types.GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=8192,
                    top_k=40,
                    top_p=0.95,
                    safety_settings=safety_settings,
                    thinking_config=types.ThinkingConfig(
                        thinking_budget=1024 if self.use_gemini_thinking else 0
                    )
                )

                response = client.models.generate_content(
                    model="gemini-2.5-pro",
                    contents=prompt,
                    config=config
                )

                final_text = None
                if response.text:
                    final_text = response.text
                elif response.candidates and response.candidates[0].content.parts:
                    final_text = " ".join([p.text for p in response.candidates[0].content.parts if p.text])

                if not final_text:
                    continue

                answer = final_text.strip().upper()

                if 'A' in answer and 'B' not in answer:
                    return 1.0
                elif 'B' in answer and 'A' not in answer:
                    return 0.0
                elif answer == 'A':
                    return 1.0
                elif answer == 'B':
                    return 0.0

                time.sleep(1)

            except Exception as e:
                if "429" in str(e) or "quota" in str(e).lower():
                    time.sleep(20)
                else:
                    time.sleep(3)

        return None

    def _predict_qwen(self, anchor: str, text_a: str, text_b: str) -> Optional[float]:
        """Qwen3-Maxé¢„æµ‹"""
        prompt = f"""You are an expert in narrative analysis. Compare three stories to determine narrative similarity.

NARRATIVE SIMILARITY:
1. **Abstract Theme** (30%): Core ideas, conflicts, motifs
2. **Course of Action** (40%): Event sequence, plot structure
3. **Outcomes** (30%): Final resolution, character fates

ANCHOR STORY:
{anchor}

CANDIDATE A:
{text_a}

CANDIDATE B:
{text_b}

Which candidate (A or B) is MORE narratively similar to the Anchor?
Respond with ONLY: A or B"""

        for retry in range(2):
            try:
                api_key = self.qwen_keys[self.qwen_key_index]
                self.qwen_key_index = (self.qwen_key_index + 1) % len(self.qwen_keys)

                time.sleep(1)
                client = OpenAI(
                    api_key=api_key,
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                )

                response = client.chat.completions.create(
                    model="qwen3-max",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2,
                    max_tokens=8192,
                    top_p=0.95,
                )

                final_text = response.choices[0].message.content

                if not final_text:
                    continue

                answer = final_text.strip().upper()

                if 'A' in answer and 'B' not in answer:
                    return 1.0
                elif 'B' in answer and 'A' not in answer:
                    return 0.0
                elif answer == 'A':
                    return 1.0
                elif answer == 'B':
                    return 0.0

                time.sleep(1)

            except Exception as e:
                if "429" in str(e) or "rate" in str(e).lower():
                    time.sleep(15)
                else:
                    time.sleep(3)

        return None

    def predict(self, anchor: str, text_a: str, text_b: str) -> Dict:
        """ä¸‰æ¨¡å‹é›†æˆé¢„æµ‹"""
        # E5é¢„æµ‹
        e5_conf = self._predict_e5(anchor, text_a, text_b)

        # Geminié¢„æµ‹
        gemini_conf = self._predict_gemini(anchor, text_a, text_b)

        # Qwené¢„æµ‹
        qwen_conf = self._predict_qwen(anchor, text_a, text_b)

        # ç»Ÿè®¡æˆåŠŸçš„æ¨¡å‹æ•°
        valid_models = []
        if e5_conf is not None:
            valid_models.append('e5')
        if gemini_conf is not None:
            valid_models.append('gemini')
        if qwen_conf is not None:
            valid_models.append('qwen')

        # æ ¹æ®å¯ç”¨æ¨¡å‹åŠ¨æ€è°ƒæ•´æƒé‡
        if len(valid_models) == 3:
            # ä¸‰ä¸ªéƒ½æˆåŠŸ
            ensemble_conf = (
                    self.e5_weight * e5_conf +
                    self.gemini_weight * gemini_conf +
                    self.qwen_weight * qwen_conf
            )
            method = 'triple_ensemble'

        elif len(valid_models) == 2:
            # ä¸¤ä¸ªæˆåŠŸ
            if 'e5' in valid_models and 'gemini' in valid_models:
                # E5 + Gemini
                total = self.e5_weight + self.gemini_weight
                ensemble_conf = (self.e5_weight / total * e5_conf +
                                 self.gemini_weight / total * gemini_conf)
                method = 'e5_gemini'

            elif 'e5' in valid_models and 'qwen' in valid_models:
                # E5 + Qwen
                total = self.e5_weight + self.qwen_weight
                ensemble_conf = (self.e5_weight / total * e5_conf +
                                 self.qwen_weight / total * qwen_conf)
                method = 'e5_qwen'

            else:  # gemini + qwen
                # Gemini + Qwen
                total = self.gemini_weight + self.qwen_weight
                ensemble_conf = (self.gemini_weight / total * gemini_conf +
                                 self.qwen_weight / total * qwen_conf)
                method = 'gemini_qwen'

        elif len(valid_models) == 1:
            # åªæœ‰ä¸€ä¸ªæˆåŠŸ
            if 'e5' in valid_models:
                ensemble_conf = e5_conf
                method = 'e5_only'
            elif 'gemini' in valid_models:
                ensemble_conf = gemini_conf
                method = 'gemini_only'
            else:
                ensemble_conf = qwen_conf
                method = 'qwen_only'
        else:
            # å…¨éƒ¨å¤±è´¥,é»˜è®¤True
            ensemble_conf = 0.51
            method = 'fallback'

        # è®¡ç®—ä¸€è‡´æ€§
        predictions = []
        if e5_conf is not None:
            predictions.append(e5_conf > 0.5)
        if gemini_conf is not None:
            predictions.append(gemini_conf > 0.5)
        if qwen_conf is not None:
            predictions.append(qwen_conf > 0.5)

        # åˆ¤æ–­æ˜¯å¦ä¸€è‡´ (è‡³å°‘2/3ä¸€è‡´)
        if len(predictions) >= 2:
            agreement = sum(predictions) >= len(predictions) / 2
        else:
            agreement = True

        return {
            'prediction': ensemble_conf > 0.5,
            'e5_confidence': e5_conf,
            'gemini_confidence': gemini_conf,
            'qwen_confidence': qwen_conf,
            'ensemble_confidence': ensemble_conf,
            'agreement': agreement,
            'method': method,
            'valid_models': len(valid_models)
        }

    def generate_submission(
            self,
            test_file: str,
            output_file: str,
            save_interval: int = 5
    ):
        """ç”Ÿæˆæäº¤æ–‡ä»¶"""
        dataset = load_dataset('json', data_files=test_file, split='train')

        results = []
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    results = [json.loads(line) for line in f if line.strip()]
            except:
                results = []

        start_idx = len(results)
        pbar = tqdm(total=len(dataset), initial=start_idx, desc="ä¸‰æ¨¡å‹Ensemble")

        stats = {
            'total': 0,
            'triple': 0,
            'double': 0,
            'single': 0,
            'fallback': 0,
            'agreement': 0,
            'disagreement': 0
        }

        for idx in range(start_idx, len(dataset)):
            item = dataset[idx]

            anchor = item.get('anchor_text') or item.get('anchor_story')
            text_a = item.get('text_a') or item.get('similar_story')
            text_b = item.get('text_b') or item.get('dissimilar_story')

            if not all([anchor, text_a, text_b]):
                results.append({
                    'text_a_is_closer': True,
                    'method': 'default'
                })
                pbar.update(1)
                continue

            # ä¸‰æ¨¡å‹é¢„æµ‹
            pred_result = self.predict(anchor, text_a, text_b)

            # ç»Ÿè®¡
            stats['total'] += 1
            if pred_result['valid_models'] == 3:
                stats['triple'] += 1
            elif pred_result['valid_models'] == 2:
                stats['double'] += 1
            elif pred_result['valid_models'] == 1:
                stats['single'] += 1
            else:
                stats['fallback'] += 1

            if pred_result['agreement']:
                stats['agreement'] += 1
            else:
                stats['disagreement'] += 1

            # ä¿å­˜ç»“æœ
            results.append({
                'text_a_is_closer': pred_result['prediction'],
                'e5_confidence': pred_result['e5_confidence'],
                'gemini_confidence': pred_result['gemini_confidence'],
                'qwen_confidence': pred_result['qwen_confidence'],
                'ensemble_confidence': pred_result['ensemble_confidence'],
                'agreement': pred_result['agreement'],
                'method': pred_result['method'],
                'valid_models': pred_result['valid_models']
            })

            # å®šæœŸä¿å­˜
            if (idx + 1) % save_interval == 0:
                self._save_results(results, output_file)

            # æ›´æ–°è¿›åº¦
            pbar.set_postfix({
                '3æ¨¡å‹': stats['triple'],
                '2æ¨¡å‹': stats['double'],
                'ä¸€è‡´': f"{stats['agreement']}/{stats['total']}"
            })
            pbar.update(1)

        pbar.close()
        self._save_results(results, output_file)

        # æ‰“å°ç»Ÿè®¡
        if stats['total'] > 0:
            print(f"\n{'=' * 60}")
            print("ğŸ“Š ä¸‰æ¨¡å‹Ensembleç»Ÿè®¡:")
            print(f"{'=' * 60}")
            print(f"æ€»æ ·æœ¬: {stats['total']}")
            print(f"ä¸‰æ¨¡å‹æˆåŠŸ: {stats['triple']} ({stats['triple'] / stats['total'] * 100:.1f}%)")
            print(f"ä¸¤æ¨¡å‹æˆåŠŸ: {stats['double']} ({stats['double'] / stats['total'] * 100:.1f}%)")
            print(f"å•æ¨¡å‹æˆåŠŸ: {stats['single']} ({stats['single'] / stats['total'] * 100:.1f}%)")
            print(f"é™çº§å¤„ç†: {stats['fallback']}")
            print(f"\nä¸€è‡´æ€§: {stats['agreement']} ({stats['agreement'] / stats['total'] * 100:.1f}%)")
            print(f"ä¸ä¸€è‡´: {stats['disagreement']} ({stats['disagreement'] / stats['total'] * 100:.1f}%)")

            a_count = sum(1 for r in results if r['text_a_is_closer'])
            print(f"\næœ€ç»ˆåˆ†å¸ƒ: A={a_count}, B={len(results) - a_count}")
            print(f"æäº¤æ–‡ä»¶: {output_file}")
            print(f"{'=' * 60}\n")

    def _save_results(self, results: List[Dict], filepath: str):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)

        def to_python(val):
            if val is None:
                return None
            if hasattr(val, 'item'):
                return val.item()
            if hasattr(val, 'tolist'):
                return val.tolist()
            return val

        # ä¿å­˜æäº¤æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            for r in results:
                f.write(json.dumps({
                    'text_a_is_closer': to_python(r['text_a_is_closer'])
                }, ensure_ascii=False) + '\n')

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detail_file = filepath.replace('.jsonl', '_detail.jsonl')
        with open(detail_file, 'w', encoding='utf-8') as f:
            for r in results:
                clean = {k: to_python(v) for k, v in r.items()}
                f.write(json.dumps(clean, ensure_ascii=False) + '\n')


def load_api_keys(key_file: str) -> List[str]:
    """åŠ è½½API keys"""
    with open(key_file, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]


def main():
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    # æ¨¡å‹è·¯å¾„
    E5_MODEL_PATH = f"{PROJECT_ROOT}/output/GoodModel/E5_0.695"

    # API keys
    GEMINI_KEY_FILE = f"{PROJECT_ROOT}/config/gemini_api_keys.txt"
    QWEN_KEY_FILE = f"{PROJECT_ROOT}/config/qwen_api_keys.txt"

    GEMINI_KEYS = load_api_keys(GEMINI_KEY_FILE)
    QWEN_KEYS = load_api_keys(QWEN_KEY_FILE)

    # æµ‹è¯•é›†
    TEST_FILE = f"{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl"
    # TEST_FILE = f"{PROJECT_ROOT}/test/track_a.jsonl"  # çœŸå®æäº¤

    # è¾“å‡º
    OUTPUT_DIR = f"{PROJECT_ROOT}/submissions/triple_ensemble"
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print(f"{'=' * 60}")
    print("ğŸ¯ ä¸‰æ¨¡å‹Ensemble: E5 + Gemini + Qwen")
    print(f"{'=' * 60}")
    print(f"E5æ¨¡å‹: {E5_MODEL_PATH}")
    print(f"Gemini Keys: {len(GEMINI_KEYS)}ä¸ª")
    print(f"Qwen Keys: {len(QWEN_KEYS)}ä¸ª")
    print(f"æµ‹è¯•é›†: {TEST_FILE}")
    print(f"{'=' * 60}\n")

    # åˆ›å»ºEnsemble
    ensemble = TripleModelEnsemble(
        e5_model_path=E5_MODEL_PATH,
        gemini_api_keys=GEMINI_KEYS,
        qwen_api_keys=QWEN_KEYS,
        e5_weight=0.3,
        gemini_weight=0.4,
        qwen_weight=0.3,
        use_gemini_thinking=True
    )

    # ç”Ÿæˆé¢„æµ‹
    ensemble.generate_submission(
        test_file=TEST_FILE,
        output_file=f"{OUTPUT_DIR}/track_a.jsonl",
        save_interval=5
    )

    print("\nâœ… å®Œæˆ!")


if __name__ == "__main__":
    main()