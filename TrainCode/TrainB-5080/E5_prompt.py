"""
ç»ˆææ–¹æ¡ˆ: æ‰¾å‡ºE5æœ€ä½³çš„å•ä¸ªé…ç½®
æµ‹è¯•"passage"å’Œ"query_similar"åœ¨ä¸åŒå‚æ•°ä¸‹çš„è¡¨ç°
"""
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from datasets import load_dataset
import numpy as np


def test_e5_with_variations(data_path):
    """
    æµ‹è¯•E5çš„ä¸åŒé…ç½®ç»„åˆ
    1. ä¸åŒprompt
    2. ä¸åŒnormalizeæ–¹å¼
    3. ä¸åŒç›¸ä¼¼åº¦è®¡ç®—
    """
    print("ğŸ”¬ E5-large æ·±åº¦ä¼˜åŒ–")
    print("=" * 70)

    # åŠ è½½æ•°æ®
    dataset = load_dataset('json', data_files=data_path, split='train')

    clean_data = []
    for item in dataset:
        anchor = item.get('anchor_text')
        text_a = item.get('text_a')
        text_b = item.get('text_b')
        label = item.get('text_a_is_closer')

        if all([anchor, text_a, text_b, label is not None]):
            clean_data.append({
                'anchor': anchor,
                'text_a': text_a,
                'text_b': text_b,
                'label': 'A' if label else 'B'
            })

    print(f"âœ… æµ‹è¯•æ ·æœ¬: {len(clean_data)}\n")

    # åŠ è½½æ¨¡å‹
    model = SentenceTransformer('/mnt/e/model/e5-large-v2')

    # é…ç½®çŸ©é˜µ
    configs = [
        # (prompt, normalize, name)
        ('passage: ', True, 'passage_normalized'),
        ('passage: ', False, 'passage_no_norm'),
        ('query: find similar stories: ', True, 'query_similar_normalized'),
        ('query: find similar stories: ', False, 'query_similar_no_norm'),
        ('', True, 'no_prompt_normalized'),
        ('', False, 'no_prompt_no_norm'),
    ]

    results = []

    for prompt, normalize, name in configs:
        print(f"æµ‹è¯•é…ç½®: {name}")

        correct = 0
        for sample in clean_data:
            # ç¼–ç 
            texts = [prompt + t for t in [sample['anchor'], sample['text_a'], sample['text_b']]]
            embeddings = model.encode(
                texts,
                normalize_embeddings=normalize,
                show_progress_bar=False
            )

            # è®¡ç®—ç›¸ä¼¼åº¦
            sim_a = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            sim_b = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]

            pred = 'A' if sim_a > sim_b else 'B'
            if pred == sample['label']:
                correct += 1

        accuracy = correct / len(clean_data)
        results.append((name, accuracy, prompt, normalize))

        print(f"  å‡†ç¡®ç‡: {accuracy:.4f} ({correct}/{len(clean_data)})\n")

    # æ’åºæ˜¾ç¤º
    print("=" * 70)
    print("ğŸ“Š é…ç½®æ’åº")
    print("=" * 70)

    results.sort(key=lambda x: x[1], reverse=True)

    for i, (name, acc, prompt, norm) in enumerate(results, 1):
        marker = "ğŸ†" if i == 1 else f"{i}."
        print(f"{marker} {name:30s}: {acc:.4f} ({acc*100:.2f}%)")

    return results


def final_verification(best_config):
    """æœ€ç»ˆéªŒè¯æœ€ä½³é…ç½®"""
    print("\n" + "=" * 70)
    print("ğŸ¯ æœ€ä½³é…ç½®éªŒè¯")
    print("=" * 70)

    name, acc, prompt, normalize = best_config

    print(f"é…ç½®åç§°: {name}")
    print(f"Prompt: '{prompt}'")
    print(f"Normalize: {normalize}")
    print(f"å‡†ç¡®ç‡: {acc:.4f} ({acc*100:.2f}%)")
    print("=" * 70)

    # ä¿å­˜æœ€ç»ˆé…ç½®
    import json

    config = {
        'model': 'intfloat/e5-large-v2',
        'model_path': '/mnt/e/model/e5-large-v2',
        'prompt': prompt,
        'normalize_embeddings': normalize,
        'accuracy': float(acc),
        'accuracy_percentage': f"{acc*100:.2f}%",
        'usage': f"""
model = SentenceTransformer('/mnt/e/model/e5-large-v2')

def predict(anchor, text_a, text_b):
    texts = ['{prompt}' + t for t in [anchor, text_a, text_b]]
    embeddings = model.encode(texts, normalize_embeddings={normalize})
    
    sim_a = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    sim_b = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]
    
    return 'A' if sim_a > sim_b else 'B'
        """
    }

    output_path = '/mnt/e/Code/python/Narrative-Similarity-Task/output/FINAL_BEST_CONFIG.json'
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)

    print(f"\nğŸ’¾ æœ€ç»ˆé…ç½®å·²ä¿å­˜: {output_path}")

    return config


def create_production_code(config):
    """ç”Ÿæˆç”Ÿäº§ä»£ç """

    code = f'''"""
ç”Ÿäº§ç¯å¢ƒæœ€ä½³é…ç½®
æ¨¡å‹: E5-large-v2
å‡†ç¡®ç‡: {config['accuracy_percentage']}
"""
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class NarrativeSimilarityModel:
    """å™äº‹ç›¸ä¼¼åº¦é¢„æµ‹æ¨¡å‹ - æœ€ä¼˜é…ç½®"""
    
    def __init__(self):
        self.model = SentenceTransformer('{config['model_path']}')
        self.prompt = "{config['prompt']}"
        self.normalize = {config['normalize_embeddings']}
    
    def predict(self, anchor: str, text_a: str, text_b: str) -> str:
        """
        é¢„æµ‹å“ªä¸ªæ–‡æœ¬ä¸anchoræ›´ç›¸ä¼¼
        
        Returns:
            'A' or 'B'
        """
        texts = [self.prompt + t for t in [anchor, text_a, text_b]]
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=self.normalize,
            show_progress_bar=False
        )
        
        sim_a = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        sim_b = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]
        
        return 'A' if sim_a > sim_b else 'B'
    
    def get_similarity_scores(self, anchor: str, text_a: str, text_b: str):
        """è·å–è¯¦ç»†çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        texts = [self.prompt + t for t in [anchor, text_a, text_b]]
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=self.normalize,
            show_progress_bar=False
        )
        
        sim_a = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        sim_b = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]
        
        return {{
            'similarity_a': float(sim_a),
            'similarity_b': float(sim_b),
            'prediction': 'A' if sim_a > sim_b else 'B',
            'confidence': abs(sim_a - sim_b)
        }}


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = NarrativeSimilarityModel()
    
    anchor = "A hero defeats a dragon and saves a princess."
    text_a = "A knight slays a monster and rescues a maiden."
    text_b = "A warrior loses a battle and dies."
    
    prediction = model.predict(anchor, text_a, text_b)
    print(f"é¢„æµ‹ç»“æœ: {{prediction}}")
    
    scores = model.get_similarity_scores(anchor, text_a, text_b)
    print(f"è¯¦ç»†åˆ†æ•°: {{scores}}")
'''

    output_path = '/mnt/e/Code/python/Narrative-Similarity-Task/output/production_model.py'
    with open(output_path, 'w') as f:
        f.write(code)

    print(f"ğŸ“ ç”Ÿäº§ä»£ç å·²ç”Ÿæˆ: {output_path}")


def main():
    print("ğŸ¯ E5-large ç»ˆæä¼˜åŒ–")
    print("=" * 70)

    data_path = '/mnt/e/Code/python/Narrative-Similarity-Task/TrainingSet1/dev_track_a.jsonl'

    # æµ‹è¯•æ‰€æœ‰é…ç½®
    results = test_e5_with_variations(data_path)

    # éªŒè¯æœ€ä½³é…ç½®
    best_config = results[0]
    config = final_verification(best_config)

    # ç”Ÿæˆç”Ÿäº§ä»£ç 
    create_production_code(config)

    print("\n" + "=" * 70)
    print("ğŸ† æœ€ç»ˆç»“æœ")
    print("=" * 70)
    print(f"æœ€ä½³å‡†ç¡®ç‡: {config['accuracy_percentage']}")
    print(f"é…ç½®: E5-large + '{config['prompt']}' + normalize={config['normalize_embeddings']}")
    print("=" * 70)


if __name__ == "__main__":
    main()