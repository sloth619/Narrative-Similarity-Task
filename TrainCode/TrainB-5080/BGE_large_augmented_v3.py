"""
Track Bè®­ç»ƒ - BGE (ä½¿ç”¨ç®€å•å¢å¼ºæ•°æ®)
æ— æ•°æ®æ³„éœ²ç‰ˆæœ¬ - å¯¹æ‰€æœ‰è®­ç»ƒæ•°æ®è¿›è¡ŒWordNetå¢å¼º
"""
import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses
from datasets import load_dataset, Dataset, concatenate_datasets
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave
from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments
import torch


def build_triplets_from_track_a(data_path):
    """ä»Track Aæ•°æ®æ„å»ºè®­ç»ƒæ ·æœ¬"""
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    for item in dataset:
        anchor = item.get('anchor_text')
        text_a = item.get('text_a')
        text_b = item.get('text_b')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            continue

        # ç¡®å®šæ­£æ ·æœ¬
        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
        else:
            positive = text_a

        # æ„å»ºå¯¹æ¯”å­¦ä¹ æ ·æœ¬å¯¹
        train_data.append({'sentence1': anchor, 'sentence2': positive})
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})

    return Dataset.from_list(train_data)


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - BGE (ç®€å•å¢å¼ºæ•°æ®ç‰ˆæœ¬)")
    print("âœ… ä½¿ç”¨WordNetå¢å¼ºæ‰€æœ‰è®­ç»ƒæ•°æ®,æ— æ•°æ®æ³„éœ²\n")
    print("=" * 60)

    # === è·¯å¾„é…ç½® ===
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    MODEL_NAME = '/mnt/e/model/BGE-large-en-v1.5'
    OUTPUT_PATH = f'{PROJECT_ROOT}/output/track_b_bge_simple_augmented_5080'
    os.makedirs(OUTPUT_PATH, exist_ok=True)

    DEV_TRACK_A = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'
    DEV_TRACK_B = f'{PROJECT_ROOT}/TrainingSet1/dev_track_b.jsonl'

    # ğŸŒŸ å…³é”®: ä½¿ç”¨ç®€å•å¢å¼ºåçš„æ•°æ®
    AUGMENTED_TRAIN_DATA = f'{PROJECT_ROOT}/TrainingSet_simple_augmented/augmented_all_data.jsonl'

    # === åŠ è½½æ¨¡å‹ ===
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {MODEL_NAME}")
    model = SentenceTransformer(MODEL_NAME)

    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   Embeddingç»´åº¦: {model.get_sentence_embedding_dimension()}")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # === åŠ è½½æ•°æ® ===
    print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")

    # 1. ç®€å•å¢å¼ºåçš„è®­ç»ƒæ•°æ® (1900 + 3800 = 5700)
    print(f"1. åŠ è½½ç®€å•å¢å¼ºæ•°æ®...")
    augmented_dataset = build_triplets_from_track_a(AUGMENTED_TRAIN_DATA)
    print(f"   å¢å¼ºæ•°æ®: {len(augmented_dataset)} ä¸ªæ ·æœ¬")

    # 2. Dev_bæ•°æ® (å¯é€‰)
    print(f"2. åŠ è½½Dev_bæ•°æ®...")
    dev_b_dataset = build_triplets_from_track_a(DEV_TRACK_B)
    print(f"   Dev_b: {len(dev_b_dataset)} ä¸ªæ ·æœ¬")

    # 3. ç»„åˆæ•°æ®
    train_dataset = concatenate_datasets([augmented_dataset, dev_b_dataset])
    print(f"\nâœ… æ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    # === æŸå¤±å‡½æ•° ===
    print(f"\nâš™ï¸  é…ç½®è®­ç»ƒ...")
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)
    print(f"   æŸå¤±å‡½æ•°: MultipleNegativesRankingLoss")

    # === è¯„ä¼°å™¨ (dev setå®Œå…¨ç‹¬ç«‹) ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="bge_simple_augmented",
        data_path=DEV_TRACK_A,  # âœ… dev setå®Œå…¨ç‹¬ç«‹
        batch_size=8
    )
    print(f"   è¯„ä¼°å™¨: TrackB_Accuracy_Evaluator (ç‹¬ç«‹çš„dev set)")

    # === è®­ç»ƒé…ç½® ===
    EPOCHS = 5
    BATCH_SIZE = 8
    LEARNING_RATE = 2e-5

    training_args = SentenceTransformerTrainingArguments(
        output_dir=OUTPUT_PATH,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=1,
        learning_rate=LEARNING_RATE,
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        logging_steps=50,
        metric_for_best_model="eval_evaluator",
        bf16=True,
        report_to="none",
    )

    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ¨¡å‹: BGE-large-en-v1.5")
    print(f"   æ•°æ®: ç®€å•å¢å¼ºç‰ˆæœ¬ (WordNetå¯¹æ‰€æœ‰æ•°æ®å¢å¼º)")
    print(f"   æ•°æ®é‡: åŸå§‹1900 â†’ å¢å¼º5700 (+200%)")
    print(f"   æ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Learning rate: {LEARNING_RATE}")
    print(f"   Epochs: {EPOCHS}")
    print(f"   é¢„æœŸæå‡: 0.66 â†’ 0.67-0.69 (çœŸå®å¯é )")

    print(f"\nğŸ” ä¸ä¹‹å‰ç‰ˆæœ¬å¯¹æ¯”:")
    print(f"   Baseline (åŸå§‹æ•°æ®): 0.66")
    print(f"   æœ‰æ³„éœ²ç‰ˆæœ¬ (devå›°éš¾æ ·æœ¬): 0.72 (è™šé«˜)")
    print(f"   æ— æ³„éœ²å¤±è´¥ç‰ˆæœ¬ (è®­ç»ƒé›†å›°éš¾æ ·æœ¬): 0.62 (å¤±è´¥)")
    print(f"   âœ… æœ¬ç‰ˆæœ¬ (ç®€å•å¢å¼º): é¢„æœŸ 0.67-0.69 (çœŸå®)")

    # === åˆ›å»ºTrainer ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    # === å¼€å§‹è®­ç»ƒ ===
    print(f"\n{'=' * 60}")
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ (ç®€å•å¢å¼ºç‰ˆæœ¬)...")
    print(f"{'=' * 60}\n")

    trainer.train()

    # === ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    final_model_path = os.path.join(OUTPUT_PATH, 'final_model')
    model.save(final_model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

    # === è®­ç»ƒå®Œæˆæ€»ç»“ ===
    print(f"\n{'=' * 60}")
    print(f"âœ… è®­ç»ƒå®Œæˆ (ç®€å•å¢å¼ºç‰ˆæœ¬)!")
    print(f"{'=' * 60}")
    print(f"\nğŸ“Š å®éªŒæ€»ç»“:")
    print(f"   æ–¹æ³•: WordNetåŒä¹‰è¯æ›¿æ¢å¢å¼º")
    print(f"   æ•°æ®: å¯¹æ‰€æœ‰1900ä¸ªæ ·æœ¬å¢å¼º,ç”Ÿæˆ3800ä¸ªå˜ä½“")
    print(f"   æ€»æ•°æ®é‡: 5700ä¸ªæ ·æœ¬")
    print(f"   æ•°æ®æ³„éœ²: âœ… æ—  (dev setå®Œå…¨ç‹¬ç«‹)")

    print(f"\nğŸ“ˆ é¢„æœŸæ€§èƒ½:")
    print(f"   Dev set: 0.67-0.69")
    print(f"   Test set: 0.67-0.69 (åº”è¯¥æ¥è¿‘dev set)")
    print(f"   çœŸå®æå‡: +0.01-0.03 (ç›¸æ¯”baseline 0.66)")

    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„æœ€ä½³å‡†ç¡®ç‡")
    print(f"   2. å¦‚æœè¾¾åˆ°0.67-0.69,ç”Ÿæˆæäº¤æ–‡ä»¶")
    print(f"   3. åœ¨CodaLab test setä¸ŠéªŒè¯")
    print(f"   4. å¯¹æ¯”testå’Œdevçš„å‡†ç¡®ç‡,ç¡®è®¤æ— è¿‡æ‹Ÿåˆ")

    print(f"\nğŸ“ æ¨¡å‹ä½ç½®:")
    print(f"   æœ€ä½³checkpoint: {OUTPUT_PATH}")
    print(f"   æœ€ç»ˆæ¨¡å‹: {final_model_path}")

    print(f"\n{'=' * 60}")

    # === é¢å¤–ä¿¡æ¯ ===
    print(f"\nğŸ“ å­¦ä¹ ç¬”è®°:")
    print(f"   âœ… ç®€å•å¢å¼ºæ¯”æŒ‘é€‰å›°éš¾æ ·æœ¬æ›´ç¨³å®š")
    print(f"   âœ… å¢åŠ æ•°æ®é‡(3å€)æœ‰åŠ©äºæå‡æ€§èƒ½")
    print(f"   âœ… WordNetæä¾›é«˜è´¨é‡åŒä¹‰è¯")
    print(f"   âœ… å®Œå…¨é¿å…æ•°æ®æ³„éœ²,ç»“æœçœŸå®å¯é ")
    print(f"\n{'=' * 60}")


if __name__ == "__main__":
    main()