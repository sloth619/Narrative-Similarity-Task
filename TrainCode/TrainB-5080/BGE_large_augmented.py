"""
Track Bè®­ç»ƒ - BGE-large-en-v1.5 (ä½¿ç”¨ä¼˜åŒ–åçš„æ•°æ®)
"""
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses
from datasets import load_dataset, Dataset, concatenate_datasets
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave
from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments
import torch


def build_triplets_from_track_a(data_path):
    """
    ä»Track Aæ•°æ®æ„å»ºè®­ç»ƒæ ·æœ¬
    ä½¿ç”¨å¯¹æ¯”å­¦ä¹ çš„ä¸‰å…ƒç»„æ ¼å¼
    """
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    for item in dataset:
        anchor = item.get('anchor_text')
        text_a = item.get('text_a')
        text_b = item.get('text_b')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            continue

        # ç¡®å®šæ­£æ ·æœ¬
        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
            negative = text_b if label_a_closer else text_a
        else:
            positive = text_a
            negative = text_b

        # æ„å»ºå¯¹æ¯”å­¦ä¹ æ ·æœ¬å¯¹
        train_data.append({'sentence1': anchor, 'sentence2': positive})
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})

    return Dataset.from_list(train_data)


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - BGE (ä½¿ç”¨ä¼˜åŒ–åçš„æ•°æ®)")
    print("ç›®æ ‡: 0.66 â†’ 0.67-0.68\n")
    print("="*60)

    # === è·¯å¾„é…ç½® ===
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    # æ¨¡å‹è·¯å¾„
    MODEL_NAME = '/mnt/e/model/BGE-large-en-v1.5'

    # è¾“å‡ºè·¯å¾„ (æ–°çš„å®éªŒåç§°)
    OUTPUT_PATH = f'{PROJECT_ROOT}/output/track_b_bge_optimized_5080'
    os.makedirs(OUTPUT_PATH, exist_ok=True)

    # æ•°æ®é›†è·¯å¾„
    DEV_TRACK_A = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'
    DEV_TRACK_B = f'{PROJECT_ROOT}/TrainingSet1/dev_track_b.jsonl'

    # ğŸŒŸ å…³é”®: ä½¿ç”¨ä¼˜åŒ–åçš„æ•°æ®!
    OPTIMIZED_TRAIN_DATA = f'{PROJECT_ROOT}/TrainingSet_optimized/augmented_training_data.jsonl'

    # === åŠ è½½æ¨¡å‹ ===
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {MODEL_NAME}")
    model = SentenceTransformer(MODEL_NAME)

    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   Embeddingç»´åº¦: {model.get_sentence_embedding_dimension()}")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # === åŠ è½½æ•°æ® ===
    print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")

    # 1. ä¼˜åŒ–åçš„è®­ç»ƒæ•°æ® (åŸå§‹1900 + å›°éš¾æ ·æœ¬150)
    print(f"1. åŠ è½½ä¼˜åŒ–åçš„è®­ç»ƒæ•°æ®...")
    optimized_dataset = build_triplets_from_track_a(OPTIMIZED_TRAIN_DATA)
    print(f"   ä¼˜åŒ–æ•°æ®: {len(optimized_dataset)} ä¸ªæ ·æœ¬")

    # 2. Dev_bæ•°æ® (å¯é€‰,ä½œä¸ºé¢å¤–è®­ç»ƒæ•°æ®)
    print(f"2. åŠ è½½Dev_bæ•°æ®...")
    dev_b_dataset = build_triplets_from_track_a(DEV_TRACK_B)
    print(f"   Dev_b: {len(dev_b_dataset)} ä¸ªæ ·æœ¬")

    # 3. ç»„åˆæ•°æ®
    train_dataset = concatenate_datasets([optimized_dataset, dev_b_dataset])
    print(f"\nâœ… æ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    # === æŸå¤±å‡½æ•° ===
    print(f"\nâš™ï¸  é…ç½®è®­ç»ƒ...")
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)
    print(f"   æŸå¤±å‡½æ•°: MultipleNegativesRankingLoss")

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="bge_optimized",
        data_path=DEV_TRACK_A,
        batch_size=8
    )
    print(f"   è¯„ä¼°å™¨: TrackB_Accuracy_Evaluator")

    # === è®­ç»ƒé…ç½® ===
    EPOCHS = 5
    BATCH_SIZE = 8
    LEARNING_RATE = 2e-5

    training_args = SentenceTransformerTrainingArguments(
        output_dir=OUTPUT_PATH,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=1,
        learning_rate=LEARNING_RATE,
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        logging_steps=50,
        metric_for_best_model="eval_evaluator",
        bf16=True,
        report_to="none",
    )

    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ¨¡å‹: BGE-large-en-v1.5")
    print(f"   æ•°æ®: ä¼˜åŒ–åæ•°æ® (å«å›°éš¾æ ·æœ¬å¢å¼º)")
    print(f"   æ€»æ ·æœ¬: {len(train_dataset):,}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Learning rate: {LEARNING_RATE}")
    print(f"   Epochs: {EPOCHS}")
    print(f"   é¢„æœŸæå‡: 0.66 â†’ 0.67-0.68")

    # === åˆ›å»ºTrainer ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    # === å¼€å§‹è®­ç»ƒ ===
    print(f"\n{'='*60}")
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"{'='*60}\n")

    trainer.train()

    # === ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    final_model_path = os.path.join(OUTPUT_PATH, 'final_model')
    model.save(final_model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

    # === è®­ç»ƒå®Œæˆæ€»ç»“ ===
    print(f"\n{'='*60}")
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"{'='*60}")
    print(f"\nğŸ“Š å®éªŒå¯¹æ¯”:")
    print(f"   Baseline (åŸå§‹æ•°æ®): 0.66")
    print(f"   Optimized (æœ¬æ¬¡): å¾…æµ‹è¯•")
    print(f"   é¢„æœŸæå‡: +0.01-0.02")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. åœ¨dev setä¸Šæµ‹è¯•æ–°æ¨¡å‹")
    print(f"   2. å¦‚æœè¾¾åˆ°0.67-0.68,ç”Ÿæˆæäº¤æ–‡ä»¶")
    print(f"   3. æäº¤åˆ°CodaLabéªŒè¯")
    print(f"\nğŸ“ æ¨¡å‹ä½ç½®:")
    print(f"   æœ€ä½³checkpoint: {OUTPUT_PATH}")
    print(f"   æœ€ç»ˆæ¨¡å‹: {final_model_path}")
    print(f"\n{'='*60}")


if __name__ == "__main__":
    main()