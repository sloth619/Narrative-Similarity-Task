"""
Track Bè®­ç»ƒ - Qwen3-Embedding-8B (WSL on 5080)
ä½¿ç”¨Syntheticæ•°æ® + QLoRAå¾®è°ƒ - ä¼˜åŒ–æ˜¾å­˜ç‰ˆæœ¬
"""
import os

# è§£å†³tokenizersè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses, models
from datasets import load_dataset, Dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave

from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

import torch
from transformers import BitsAndBytesConfig
from peft import LoraConfig, TaskType, prepare_model_for_kbit_training


def build_triplets_from_track_a(data_path):
    """ä»Track Aæ„å»ºè®­ç»ƒæ•°æ®"""
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    for item in dataset:
        anchor = item.get('anchor_text') or item.get('anchor_story')
        text_a = item.get('text_a') or item.get('similar_story')
        text_b = item.get('text_b') or item.get('dissimilar_story')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            continue

        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
        else:
            positive = text_a

        train_data.append({'sentence1': anchor, 'sentence2': positive})
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})

    return Dataset.from_list(train_data)


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - Qwen3-Embedding-8B with QLoRA (WSL on 5080)...")
    print("ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ç‰ˆæœ¬")

    # === WSLè·¯å¾„é…ç½® ===
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    model_name = '/mnt/e/model/Qwen3-Embedding-8B'
    output_path = f'{PROJECT_ROOT}/output/track_b_qwen3_8B_qlora_wsl'
    os.makedirs(output_path, exist_ok=True)

    dev_track_a_path = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'
    synthetic_data_path = f'{PROJECT_ROOT}/TrainingSet1/synthetic_data_for_contrastive_learning.jsonl'

    # === æ„å»ºæ¨¡å‹ with QLoRA ===
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    print("ä½¿ç”¨4-bité‡åŒ–é…ç½®...")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    word_embedding_model = models.Transformer(
        model_name,
        tokenizer_args={'padding_side': 'left'},
        model_args={
            "quantization_config": bnb_config,
            "device_map": "auto",
            "attn_implementation": "flash_attention_2",
        }
    )

    # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    word_embedding_model.auto_model = prepare_model_for_kbit_training(
        word_embedding_model.auto_model,
        use_gradient_checkpointing=True
    )

    embedding_dim = word_embedding_model.get_word_embedding_dimension()
    print(f"Embeddingç»´åº¦: {embedding_dim}")

    pooling_model = models.Pooling(
        word_embedding_dimension=embedding_dim,
        pooling_mode='lasttoken'
    )

    model = SentenceTransformer(
        modules=[word_embedding_model, pooling_model],
        device='cuda'
    )

    # === LoRAé…ç½® - å‡å°rå€¼èŠ‚çœæ˜¾å­˜ ===
    print("\né…ç½®LoRAé€‚é…å™¨...")
    lora_config = LoraConfig(
        r=32,
        lora_alpha=64,
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )

    model.add_adapter(lora_config)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_param = sum(p.numel() for p in model.parameters())
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {all_param:,} ({100 * trainable_params / all_param:.2f}%)")

    # === åŠ è½½æ•°æ® ===
    print("\nä»syntheticæ•°æ®æ„å»ºè®­ç»ƒé›†...")
    train_dataset = build_triplets_from_track_a(synthetic_data_path)
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    # === æŸå¤±å‡½æ•° ===
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="qwen3_8B_synthetic",
        data_path=dev_track_a_path,
        batch_size=8
    )

    # === ä¼˜åŒ–è®­ç»ƒé…ç½® ===
    epochs = 5

    training_args = SentenceTransformerTrainingArguments(
        output_dir=output_path,
        num_train_epochs=epochs,
        per_device_train_batch_size=16,
        gradient_accumulation_steps=4,
        learning_rate=5e-7,
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        logging_steps=50,
        metric_for_best_model="eval_evaluator",
        bf16=True,
        optim="adamw_8bit",
        gradient_checkpointing=True,
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        max_grad_norm=0.3,
    )

    print(f"\nå¼€å§‹è®­ç»ƒ:")
    print(f"  - æ¨¡å‹: Qwen3-Embedding-8B with QLoRA")
    print(f"  - Batch size: {training_args.per_device_train_batch_size} (å‡å°ä»¥èŠ‚çœæ˜¾å­˜)")
    print(f"  - Gradient Accumulation: {training_args.gradient_accumulation_steps}")
    print(f"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"  - Learning rate: {training_args.learning_rate}")
    print(f"  - Epochs: {epochs}")
    print(f"  - LoRA r: {lora_config.r} (å‡å°ä»¥èŠ‚çœæ˜¾å­˜)")
    print(f"  - Gradient Checkpointing: âœ…")

    # === æ¸…ç†æ˜¾å­˜ ===
    torch.cuda.empty_cache()

    # === è®­ç»ƒ ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    print("\nå¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # === ä¿å­˜ ===
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    try:
        model.save(output_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"å®Œæ•´æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        lora_adapter_path = os.path.join(output_path, "lora_adapter")
        model[0].auto_model.save_pretrained(lora_adapter_path)
        print(f"âœ… LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {lora_adapter_path}")

    print("\nâœ… è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()