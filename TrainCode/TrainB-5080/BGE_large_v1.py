"""
Track Bè®­ç»ƒ - BGE-large-en-v1.5 Baseline (ä¼˜åŒ–ç‰ˆ)
âœ… å¢åŠ é›¶æ ·æœ¬æµ‹è¯•
âœ… å¢åŠ è´Ÿæ ·æœ¬æŒ–æ˜
âœ… ä¼˜åŒ–æ•°æ®æ„å»ºç­–ç•¥
âœ… é¢„æœŸå‡†ç¡®ç‡: 66-69%
"""
import os
import gc
import torch

# æ¸…ç†æ˜¾å­˜
torch.cuda.empty_cache()
gc.collect()

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses
from datasets import load_dataset, Dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave

from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments


def build_triplets_from_track_a(data_path):
    """ä»Track Aæ„å»ºè®­ç»ƒæ•°æ® - æ”¹è¿›ç‰ˆ

    æ”¹è¿›ç‚¹:
    1. åŒæ—¶æ·»åŠ æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬å¯¹
    2. å¢åŠ éš¾è´Ÿæ ·æœ¬(label_a_closeræ ‡æ³¨çš„é”™è¯¯æ ·æœ¬)
    """
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    skipped = 0

    for item in dataset:
        anchor = item.get('anchor_text') or item.get('anchor_story') or item.get('anchor') or item.get('text')
        text_a = item.get('text_a') or item.get('similar_story') or item.get('positive')
        text_b = item.get('text_b') or item.get('dissimilar_story') or item.get('negative')
        label_a_closer = item.get('text_a_is_closer')

        # å¤„ç†dev_track_bæ ¼å¼
        if anchor and not text_a and not text_b:
            train_data.append({'sentence1': anchor, 'sentence2': anchor})
            continue

        # å¤„ç†Track Aä¸‰å…ƒç»„æ ¼å¼
        if not all([anchor, text_a, text_b]):
            skipped += 1
            continue

        # ğŸ”¥ æ”¹è¿›1: æ ¹æ®æ ‡ç­¾æ·»åŠ æ­£è´Ÿæ ·æœ¬
        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
            negative = text_b if label_a_closer else text_a

            # æ·»åŠ æ­£æ ·æœ¬å¯¹
            train_data.append({'sentence1': anchor, 'sentence2': positive})

            # ğŸ”¥ æ–°å¢: æ·»åŠ éš¾è´Ÿæ ·æœ¬å¯¹(é€šè¿‡æ ‡ç­¾æ˜ç¡®åŒºåˆ†)
            # æ³¨æ„:è¿™é‡Œä¸ç›´æ¥åŠ negative,è€Œæ˜¯è®©MultipleNegativesRankingLossåœ¨batchå†…è‡ªåŠ¨æŒ–æ˜

        else:
            # æ²¡æœ‰æ ‡ç­¾æ—¶,é»˜è®¤text_aæ˜¯æ­£æ ·æœ¬
            positive = text_a
            train_data.append({'sentence1': anchor, 'sentence2': positive})

        # æ·»åŠ è‡ªå¯¹æ¯”æ ·æœ¬(å¢å¼ºé²æ£’æ€§)
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})

    if skipped > 0:
        print(f"     âš ï¸ è·³è¿‡äº† {skipped} æ¡æ•°æ®")

    return Dataset.from_list(train_data)


def evaluate_zero_shot(model, data_path):
    """ğŸ”¥ æ–°å¢: è¯„ä¼°é›¶æ ·æœ¬æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ” é›¶æ ·æœ¬æµ‹è¯• - BGE-large-en-v1.5")
    print("=" * 60)

    dev_dataset = load_dataset('json', data_files=data_path, split='train')

    correct = 0
    total = 0

    print(f"å¼€å§‹è¯„ä¼° {len(dev_dataset)} ä¸ªä¸‰å…ƒç»„...")

    for idx, item in enumerate(dev_dataset):
        anchor = item.get('anchor_text') or item.get('anchor_story')
        text_a = item.get('text_a') or item.get('similar_story')
        text_b = item.get('text_b') or item.get('dissimilar_story')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]) or label_a_closer is None:
            continue

        # ç¼–ç 
        embeddings = model.encode(
            [anchor, text_a, text_b],
            show_progress_bar=False,
            batch_size=32
        )

        anchor_emb = embeddings[0]
        text_a_emb = embeddings[1]
        text_b_emb = embeddings[2]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim_a = torch.nn.functional.cosine_similarity(
            torch.tensor(anchor_emb).unsqueeze(0),
            torch.tensor(text_a_emb).unsqueeze(0)
        ).item()

        sim_b = torch.nn.functional.cosine_similarity(
            torch.tensor(anchor_emb).unsqueeze(0),
            torch.tensor(text_b_emb).unsqueeze(0)
        ).item()

        # é¢„æµ‹
        prediction = sim_a > sim_b

        if prediction == label_a_closer:
            correct += 1
        total += 1

        # è¿›åº¦æç¤º
        if (idx + 1) % 50 == 0:
            print(f"  å·²è¯„ä¼°: {idx + 1}/{len(dev_dataset)}, å½“å‰å‡†ç¡®ç‡: {correct / total:.2%}")

    accuracy = correct / total if total > 0 else 0
    print(f"\nâœ… é›¶æ ·æœ¬å‡†ç¡®ç‡: {accuracy:.4f} ({correct}/{total})")

    return accuracy


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - BGE-large-en-v1.5 Baseline (ä¼˜åŒ–ç‰ˆ)...")

    # === æ¸…ç†æ˜¾å­˜ ===
    print("\næ¸…ç†GPUæ˜¾å­˜...")
    torch.cuda.empty_cache()
    gc.collect()
    print(f"âœ… æ˜¾å­˜å·²æ¸…ç†")

    # === è·¯å¾„é…ç½® ===
    model_name = 'E:/model/BGE-large-en-v1.5'
    output_path = '../../output/track_b_bge_large_en_v15_optimized'
    os.makedirs(output_path, exist_ok=True)

    # === åŠ è½½æ¨¡å‹ ===
    print(f"\nåŠ è½½æ¨¡å‹: {model_name}")
    print("âœ… æ— å¤æ‚ä¾èµ–,100%ç¨³å®š")

    model = SentenceTransformer(model_name, device='cuda')

    print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   æ¨¡å‹: BGE-large-en-v1.5 (335Må‚æ•°)")
    print(f"   Embeddingç»´åº¦: {model.get_sentence_embedding_dimension()}")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ğŸ”¥ æ–°å¢: é›¶æ ·æœ¬æµ‹è¯•
    print("\nğŸ“Š Step 1: é›¶æ ·æœ¬æ€§èƒ½æµ‹è¯•")
    zero_shot_acc = evaluate_zero_shot(
        model=model,
        data_path="../../TrainingSet1/dev_track_a.jsonl"
    )

    print(f"\nğŸ’¡ åˆ†æ:")
    if zero_shot_acc > 0.62:
        print(f"   ğŸ‰ é›¶æ ·æœ¬å‡†ç¡®ç‡ {zero_shot_acc:.2%} å¾ˆå¥½!")
        print(f"   é¢„æœŸå¾®è°ƒåæå‡: +4~6%")
    else:
        print(f"   é›¶æ ·æœ¬å‡†ç¡®ç‡ {zero_shot_acc:.2%}")
        print(f"   é¢„æœŸå¾®è°ƒåæå‡: +5~8%")

    # === åŠ è½½è®­ç»ƒæ•°æ® ===
    print("\n" + "=" * 60)
    print("ğŸ“š Step 2: åŠ è½½è®­ç»ƒæ•°æ®")
    print("=" * 60)

    print("1. åŠ è½½Syntheticæ•°æ®...")
    synthetic_dataset = build_triplets_from_track_a(
        '../../TrainingSet1/synthetic_data_for_contrastive_learning.jsonl'
    )
    print(f"   Synthetic: {len(synthetic_dataset)} ä¸ªæ ·æœ¬")

    print("2. åŠ è½½Dev_bæ•°æ®...")
    dev_b_dataset = build_triplets_from_track_a(
        '../../TrainingSet1/dev_track_b.jsonl'
    )
    print(f"   Dev_b: {len(dev_b_dataset)} ä¸ªæ ·æœ¬")

    from datasets import concatenate_datasets
    train_dataset = concatenate_datasets([synthetic_dataset, dev_b_dataset])

    print(f"\næ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
    print(f"  - Synthetic: {len(synthetic_dataset)} ({len(synthetic_dataset) / len(train_dataset) * 100:.1f}%)")
    print(f"  - Dev_b: {len(dev_b_dataset)} ({len(dev_b_dataset) / len(train_dataset) * 100:.1f}%)")

    # === æŸå¤±å‡½æ•° ===
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="bge_large_optimized",
        data_path="../../TrainingSet1/dev_track_a.jsonl",
        batch_size=32
    )

    # ğŸ”¥ æ”¹è¿›: è°ƒæ•´è®­ç»ƒç­–ç•¥
    epochs = 5

    training_args = SentenceTransformerTrainingArguments(
        output_dir=output_path,
        num_train_epochs=epochs,
        per_device_train_batch_size=16,
        gradient_accumulation_steps=2,

        # ğŸ”¥ æ”¹è¿›: ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
        learning_rate=2e-6,
        lr_scheduler_type="cosine",  # æ–°å¢
        warmup_ratio=0.1,

        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,

        logging_steps=20,
        logging_first_step=True,  # æ–°å¢:è®°å½•ç¬¬ä¸€æ­¥

        metric_for_best_model="eval_evaluator",
        greater_is_better=True,  # æ–°å¢:æ˜ç¡®æŒ‡å®š

        bf16=True,
        dataloader_num_workers=0,
        dataloader_pin_memory=False,

        # ğŸ”¥ æ–°å¢: æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
        max_grad_norm=1.0,
    )

    print("\n" + "=" * 60)
    print("ğŸš€ Step 3: å¼€å§‹å¾®è°ƒ")
    print("=" * 60)
    print(f"é…ç½®:")
    print(f"  - ç¡¬ä»¶: RTX 5080 (16GB)")
    print(f"  - æ¨¡å‹: BGE-large-en-v1.5 (335M)")
    print(f"  - è®­ç»ƒæ•°æ®: Synthetic + Dev_b")
    print(f"  - æ€»æ ·æœ¬: {len(train_dataset):,}")
    print(f"  - Batch size: 32")
    print(f"  - Learning rate: 2e-5 (cosine schedule)")
    print(f"  - Epochs: {epochs}")
    print(f"  - é›¶æ ·æœ¬åŸºçº¿: {zero_shot_acc:.2%}")
    print(f"\né¢„æœŸç»“æœ:")
    print(f"  - å¾®è°ƒå: 66-69% ğŸ¯")
    print(f"  - è®­ç»ƒæ—¶é—´: 30-45åˆ†é’Ÿ")

    # === è®­ç»ƒ ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    try:
        print("\nå¼€å§‹è®­ç»ƒ...\n")
        trainer.train()
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­!")
        print("ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        return
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("\nâŒ æ˜¾å­˜ä¸è¶³!")
            print("ğŸ’¡ é™ä½batch_sizeåˆ°16:")
            print("   per_device_train_batch_size=16")
        else:
            raise e
        return

    # === ä¿å­˜ ===
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(output_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")

    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š BGE-large-en-v1.5æ€§èƒ½æ€»ç»“:")
    print(f"  - é›¶æ ·æœ¬: {zero_shot_acc:.2%}")
    print(f"  - å¾®è°ƒå: æŸ¥çœ‹ä¸Šæ–¹æœ€ä½³å‡†ç¡®ç‡")
    print(f"  - é¢„æœŸæå‡: +{(0.66 - zero_shot_acc) * 100:.1f}~{(0.69 - zero_shot_acc) * 100:.1f}%")
    print(f"\nğŸ¯ æ”¹è¿›è¦ç‚¹:")
    print(f"  âœ… å¢åŠ é›¶æ ·æœ¬åŸºçº¿æµ‹è¯•")
    print(f"  âœ… ä¼˜åŒ–æ•°æ®æ„å»ºç­–ç•¥")
    print(f"  âœ… ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦")
    print(f"  âœ… æ·»åŠ æ¢¯åº¦è£å‰ª")


if __name__ == "__main__":
    main()