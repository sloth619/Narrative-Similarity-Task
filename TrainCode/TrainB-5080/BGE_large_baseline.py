"""
Track Bè®­ç»ƒ - BGE-large-en-v1.5 baseline
å›ºå®šseed=42ï¼Œä¸“æ³¨è®­ç»ƒ
"""
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses
from datasets import load_dataset, Dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave
from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments
import torch


def set_seed(seed):
    """å›ºå®šéšæœºç§å­"""
    import random
    import numpy as np
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def build_triplets_from_track_a(data_path):
    """ä»Track Aæ„å»ºè®­ç»ƒæ•°æ®"""
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    for item in dataset:
        anchor = item.get('anchor_text') or item.get('anchor_story')
        text_a = item.get('text_a') or item.get('similar_story')
        text_b = item.get('text_b') or item.get('dissimilar_story')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            continue

        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
        else:
            positive = text_a

        train_data.append({'sentence1': anchor, 'sentence2': positive})
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})

    return Dataset.from_list(train_data)


def main():
    # å›ºå®šç§å­
    SEED = 42
    set_seed(SEED)

    print(f"ğŸš€ BGE-large-en-v1.5 Full Fine-tuning - Seed {SEED}")

    # è·¯å¾„é…ç½®
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"
    model_name = '/mnt/e/model/BGE-large-en-v1.5'
    output_path = f'{PROJECT_ROOT}/output/bge_full_seed42'

    os.makedirs(output_path, exist_ok=True)

    synthetic_data_path = f'{PROJECT_ROOT}/TrainingSet1/synthetic_data_for_contrastive_learning.jsonl'
    dev_track_a_path = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'

    # åŠ è½½æ¨¡å‹
    print(f"\nåŠ è½½æ¨¡å‹...")
    model = SentenceTransformer(model_name)

    # åŠ è½½æ•°æ®
    print("\nåŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = build_triplets_from_track_a(synthetic_data_path)
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    # æŸå¤±å‡½æ•°å’Œè¯„ä¼°å™¨
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="bge_full_seed42",
        data_path=dev_track_a_path,
        batch_size=8
    )

    # è®­ç»ƒé…ç½®
    training_args = SentenceTransformerTrainingArguments(
        output_dir=output_path,
        num_train_epochs=5,
        per_device_train_batch_size=8,
        gradient_accumulation_steps=1,
        learning_rate=2e-5,
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        logging_steps=50,
        metric_for_best_model="eval_evaluator",
        greater_is_better=True,
        bf16=True,
        seed=SEED,
    )

    # è®­ç»ƒ
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    print("\nå¼€å§‹è®­ç»ƒ...\n")
    trainer.train()

    # ä¿å­˜
    print("\nä¿å­˜æ¨¡å‹...")
    model.save(output_path)
    print("âœ… å®Œæˆ!")


if __name__ == "__main__":
    main()