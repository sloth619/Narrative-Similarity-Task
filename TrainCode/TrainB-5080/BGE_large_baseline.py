"""
Track Bè®­ç»ƒ - BGE-large-en-v1.5 baseline (5080)
ä½¿ç”¨å®˜æ–¹Syntheticæ•°æ®æµ‹è¯•
"""
import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses
from datasets import load_dataset, Dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave

from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

import torch


def build_triplets_from_track_a(data_path):
    """ä»Track Aæ„å»ºè®­ç»ƒæ•°æ® (Baseline åŸå§‹é€»è¾‘)"""
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    for item in dataset:
        anchor = item.get('anchor_text') or item.get('anchor_story')
        text_a = item.get('text_a') or item.get('similar_story')
        text_b = item.get('text_b') or item.get('dissimilar_story')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            # æ³¨æ„: Baseline é€»è¾‘è·³è¿‡äº† dev_track_b çš„æ•°æ®
            continue

        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
        else:
            positive = text_a

        # --- Baseline é€»è¾‘ ---
        # ä¿ç•™äº† (anchor, positive)
        train_data.append({'sentence1': anchor, 'sentence2': positive})
        # [BUG] ä¿ç•™äº† (anchor, anchor)
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        # [BUG] ä¿ç•™äº† (positive, positive)
        train_data.append({'sentence1': positive, 'sentence2': positive})
        # ---------------------

    return Dataset.from_list(train_data)


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - BGE-large-en-v1.5 Baseline (5080)...")

    # === è·¯å¾„é…ç½® (å·²ä¿®æ”¹ä¸ºWSLç»å¯¹è·¯å¾„) ===
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    # æ¨¡å‹è·¯å¾„
    model_name = '/mnt/e/model/BGE-large-en-v1.5'

    # è¾“å‡ºè·¯å¾„
    output_path = f'{PROJECT_ROOT}/output/track_b_bge_baseline_5080_wsl'
    os.makedirs(output_path, exist_ok=True)

    # æ•°æ®é›†è·¯å¾„
    dev_track_a_path = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'
    synthetic_data_path = f'{PROJECT_ROOT}/TrainingSet1/synthetic_data_for_contrastive_learning.jsonl'
    dev_track_b_path = f'{PROJECT_ROOT}/TrainingSet1/dev_track_b.jsonl'

    # === åŠ è½½æ¨¡å‹  ===
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    model = SentenceTransformer(model_name)

    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   Embeddingç»´åº¦: {model.get_sentence_embedding_dimension()}")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # === åŠ è½½æ•°æ® ===
    print("\nåŠ è½½è®­ç»ƒæ•°æ®...")

    print("1. åŠ è½½Syntheticæ•°æ®...")
    synthetic_dataset = build_triplets_from_track_a(
        synthetic_data_path # <-- ä½¿ç”¨WSLè·¯å¾„
    )
    print(f"   Synthetic: {len(synthetic_dataset)} ä¸ªæ ·æœ¬")

    print("2. åŠ è½½Dev_bæ•°æ®...")
    dev_b_dataset = build_triplets_from_track_a(
        dev_track_b_path # <-- ä½¿ç”¨WSLè·¯å¾„
    )
    print(f"   Dev_b: {len(dev_b_dataset)} ä¸ªæ ·æœ¬")

    from datasets import concatenate_datasets
    train_dataset = concatenate_datasets([synthetic_dataset, dev_b_dataset])

    print(f"\næ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")

    # === æŸå¤±å‡½æ•° ===
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="bge_baseline",
        data_path=dev_track_a_path, # <-- ä½¿ç”¨WSLè·¯å¾„
        batch_size=8
    )

    # === è®­ç»ƒé…ç½® (BGEæ¨èå‚æ•°) ===
    epochs = 5

    training_args = SentenceTransformerTrainingArguments(
        output_dir=output_path,
        num_train_epochs=epochs,
        per_device_train_batch_size=8,
        gradient_accumulation_steps=1,
        learning_rate=2e-5,
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        logging_steps=20,
        metric_for_best_model="eval_evaluator",
        bf16=True,
    )

    print(f"\nå¼€å§‹è®­ç»ƒ:")
    print(f"  - æ¨¡å‹: BGE-large-en-v1.5")
    print(f"  - è®­ç»ƒæ•°æ®: Synthetic + Dev_b (Baseline-Bug-Logic)")
    print(f"  - æ€»æ ·æœ¬: {len(train_dataset):,}")
    print(f"  - Batch size: {training_args.per_device_train_batch_size}")
    print(f"  - Learning rate: {training_args.learning_rate}")
    print(f"  - Epochs: {epochs}")

    # === è®­ç»ƒ ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    trainer.train()

    # === ä¿å­˜ ===
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(output_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")

    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"\nBGE baselineé¢„æœŸå‡†ç¡®ç‡: 60-63%")


if __name__ == "__main__":
    main()