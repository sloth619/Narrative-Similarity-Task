"""
Track Bè®­ç»ƒ - Qwen3-Embedding-4B (V13 - RTX 5080 + 20k CMU)
"""
import os
import gc
import torch

# æ¸…ç†æ˜¾å­˜
torch.cuda.empty_cache()
gc.collect()

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from sentence_transformers import SentenceTransformer, losses, models
from datasets import load_dataset, Dataset
from train_b_evaluator import TrackB_Accuracy_Evaluator_NoSave

from sentence_transformers import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

import torch
from transformers import BitsAndBytesConfig
from peft import LoraConfig, TaskType, prepare_model_for_kbit_training


def build_triplets_baseline_logic(data_path, max_length=200):
    dataset = load_dataset('json', data_files=data_path, split='train')

    train_data = []
    skipped = 0

    for item in dataset:
        anchor = item.get('anchor_text') or item.get('anchor_story') or item.get('anchor')
        text_a = item.get('text_a') or item.get('similar_story') or item.get('positive')
        text_b = item.get('text_b') or item.get('dissimilar_story') or item.get('negative')
        label_a_closer = item.get('text_a_is_closer')

        if not all([anchor, text_a, text_b]):
            skipped += 1
            continue

        # é™åˆ¶æ–‡æœ¬é•¿åº¦ (ä¿æŒ VRAM å ç”¨å¯æ§)
        def truncate_text(text, max_words=max_length):
            words = text.split()
            if len(words) > max_words:
                return ' '.join(words[:max_words])
            return text

        anchor = truncate_text(anchor)
        text_a = truncate_text(text_a)
        text_b = truncate_text(text_b)

        if label_a_closer is not None:
            positive = text_a if label_a_closer else text_b
        else:
            positive = text_a

        # --- Baseline çš„æ•°æ®å¢å¼º (1 -> 3) ---
        train_data.append({'sentence1': anchor, 'sentence2': positive})
        train_data.append({'sentence1': anchor, 'sentence2': anchor})
        train_data.append({'sentence1': positive, 'sentence2': positive})
        # -------------------------------------

    if skipped > 0:
        print(f"  âš ï¸ è·³è¿‡äº† {skipped} æ¡æ•°æ®")

    return Dataset.from_list(train_data)


def main():
    print("ğŸš€ Track Bè®­ç»ƒ - Qwen3-4B (V13 - RTX 5080 + 20k CMU)...")

    # === è·¯å¾„é…ç½® (5080 WSL è·¯å¾„) ===
    PROJECT_ROOT = "/mnt/e/Code/python/Narrative-Similarity-Task"

    model_name = '/mnt/e/model/Qwen3-Embedding-4B'
    # V10 (bs=16, r=32, 10k data) çš„è¾“å‡ºè·¯å¾„
    v10_output_path = f'{PROJECT_ROOT}/output/track_b_qwen3_cmu_5080_v9_bs128'
    # V13 (bs=16, r=32, 20k data) çš„æ–°è¾“å‡ºè·¯å¾„
    output_path = f'{PROJECT_ROOT}/output/track_b_qwen3_cmu_5080_v13_20k_fix' # V13 æ–°åå­—
    os.makedirs(output_path, exist_ok=True)

    # æ•°æ®é›†è·¯å¾„
    dev_track_a_path = f'{PROJECT_ROOT}/TrainingSet1/dev_track_a.jsonl'
    synthetic_data_path = f'{PROJECT_ROOT}/TrainingSet1/synthetic_data_for_contrastive_learning.jsonl'
    cmu_data_path = f'{PROJECT_ROOT}/TrainingSet1/cmu_movie_triplets.jsonl'

    # === æ£€æŸ¥æ–­ç‚¹ ===
    # æˆ‘ä»¬ä» V13 è‡ªå·±çš„è·¯å¾„åŠ è½½æ–­ç‚¹
    checkpoint_path = None
    if os.path.exists(output_path):
        checkpoints = [d for d in os.listdir(output_path) if d.startswith('checkpoint-')]
        if checkpoints:
            checkpoints.sort(key=lambda x: int(x.split('-')[1]))
            checkpoint_path = os.path.join(output_path, checkpoints[-1])
            print(f"âœ… æ‰¾åˆ° V13 æ£€æŸ¥ç‚¹: {checkpoint_path}")

    # === æ„å»ºæ¨¡å‹ ===
    if checkpoint_path:
        print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹...")
        model = SentenceTransformer(checkpoint_path)
        print("âœ… æ¨¡å‹ä»æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ")
    else:
        print(f"ä»å¤´å¼€å§‹è®­ç»ƒ,åŠ è½½åŸºç¡€æ¨¡å‹: {model_name}")

        # 4-bité‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        word_embedding_model = models.Transformer(
            model_name, # <-- æ€»æ˜¯åŠ è½½åŸºç¡€æ¨¡å‹
            tokenizer_args={'padding_side': 'left'},
            model_args={
                "quantization_config": bnb_config,
                "device_map": "auto",
            }
        )

        word_embedding_model.auto_model = prepare_model_for_kbit_training(
            word_embedding_model.auto_model,
            use_gradient_checkpointing=True
        )

        embedding_dim = word_embedding_model.get_word_embedding_dimension()
        print(f"Embeddingç»´åº¦: {embedding_dim}")

        pooling_model = models.Pooling(
            word_embedding_dimension=embedding_dim,
            pooling_mode='lasttoken'
        )

        model = SentenceTransformer(
            modules=[word_embedding_model, pooling_model],
            device='cuda'
        )

        # LoRAé…ç½® (r=32 ä¿æŒä¸å˜)
        lora_config = LoraConfig(
            r=32,
            lora_alpha=64,
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        )

        print("âœ… æ— æ¡ä»¶æ·»åŠ  LoRA (r=32) é€‚é…å™¨...")
        model.add_adapter(lora_config)

        v10_best_checkpoint = os.path.join(v10_output_path, "checkpoint-246")
        if os.path.exists(v10_best_checkpoint):
             print(f"âœ… æ­£åœ¨ä» V10 æ–­ç‚¹ ({v10_best_checkpoint}) çƒ­å¯åŠ¨ LoRA æƒé‡...")
             try:
                model.load_adapter(v10_best_checkpoint, "default")
                print("âœ… V10 LoRA æƒé‡åŠ è½½æˆåŠŸ!")
                checkpoint_path = v10_best_checkpoint
             except Exception as e:
                print(f"âš ï¸ åŠ è½½ V10 LoRA æƒé‡å¤±è´¥ (å¯èƒ½æ˜¯ V10 OOM å¯¼è‡´): {e}")
                print("   å°†ä»å¤´è®­ç»ƒ...")

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_param = sum(p.numel() for p in model.parameters())
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {all_param:,} ({100 * trainable_params / all_param:.2f}%)")
    print("âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ° VRAMã€‚")


    # === åŠ è½½æ•°æ® ===
    print("\nåŠ è½½è®­ç»ƒæ•°æ® (Baseline é€»è¾‘)...")

    print("1. åŠ è½½Syntheticæ•°æ®...")
    synthetic_dataset = build_triplets_baseline_logic(
        synthetic_data_path,
        max_length=200
    )
    print(f"   Synthetic: {len(synthetic_dataset)} ä¸ªæ ·æœ¬")

    print("2. åŠ è½½CMU Movieæ•°æ®...")
    cmu_dataset = build_triplets_baseline_logic(
        cmu_data_path,
        max_length=200
    )

    # ğŸ”¥ [V12 ä¼˜åŒ–] å¢åŠ æ•°æ®é‡
    cmu_sample_size = 20000
    cmu_dataset = cmu_dataset.select(range(min(cmu_sample_size, len(cmu_dataset))))
    print(f"   CMU Movie: {len(cmu_dataset)} ä¸ªæ ·æœ¬ (å·²å¢åŠ è‡³ {cmu_sample_size})")

    from datasets import concatenate_datasets
    train_dataset = concatenate_datasets([synthetic_dataset, cmu_dataset])
    print(f"\næ€»è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}") # çº¦ 25,691

    # === æŸå¤±å‡½æ•° ===
    mnrl_loss = losses.MultipleNegativesRankingLoss(model=model)

    # === è¯„ä¼°å™¨ ===
    evaluator = TrackB_Accuracy_Evaluator_NoSave(
        name="qwen3_cmu_5080_v13_20k",
        data_path=dev_track_a_path,
        batch_size=32
    )

    # === è®­ç»ƒé…ç½® (ä¿æŒ V10 é…ç½®) ===
    epochs = 3
    current_batch_size = 24
    current_grad_steps = 6
    effective_batch_size = current_batch_size * current_grad_steps # 16 * 8 = 128
    current_learning_rate = 5e-7

    training_args = SentenceTransformerTrainingArguments(
        output_dir=output_path,
        num_train_epochs=epochs,
        per_device_train_batch_size=current_batch_size,
        gradient_accumulation_steps=current_grad_steps,

        learning_rate=current_learning_rate,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        max_grad_norm=1.0,

        optim="paged_adamw_8bit",
        adam_beta1=0.9,
        adam_beta2=0.99,
        adam_epsilon=1e-8,

        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=3,

        # ğŸ”¥ OOM ä¿®å¤
        load_best_model_at_end=False,

        logging_steps=50,
        metric_for_best_model="eval_evaluator",
        bf16=True,
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        # resume_from_checkpoint=checkpoint_path, # <-- V13 ä¸­æ–­/æ¢å¤é€»è¾‘
    )

    print(f"\nå¼€å§‹è®­ç»ƒ (V13 - 20k CMU æ•°æ®ç‰ˆ):")
    resume_from_v13_checkpoint = checkpoint_path if "v13" in str(checkpoint_path) else None
    if resume_from_v13_checkpoint:
        print(f"  - âœ… æ–­ç‚¹ç»­ä¼ : {resume_from_v13_checkpoint}")
    elif "v9" in str(checkpoint_path): # (v9_bs128 æ˜¯ V10)
        print(f"  - ğŸ”¥ V10 çƒ­å¯åŠ¨: {checkpoint_path}")
    else:
        print(f"  - ğŸ†• ä»å¤´è®­ç»ƒ")

    print(f"  - æ¨¡å‹: Qwen3-Embedding-4B (4-bit + LoRA r=32)")
    print(f"  - æ€»æ ·æœ¬: {len(train_dataset):,}")
    print(f"  - BS (æœ‰æ•ˆ): {effective_batch_size}")
    print(f"  - å­¦ä¹ ç‡: {current_learning_rate}")
    print(f"  - é¢„æœŸ: è®­ç»ƒæ—¶é—´æ›´é•¿, æ€§èƒ½æœ‰æœ›è¶…è¶Š 65.0%")

    # === è®­ç»ƒ ===
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        loss=mnrl_loss,
        evaluator=evaluator,
    )

    try:
        trainer.train(resume_from_checkpoint=resume_from_v13_checkpoint)
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­!")
        print("ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜,å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­è®­ç»ƒ")
        return
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("\nâŒ æ˜¾å­˜ä¸è¶³ (OOM)! r=32 + 20k æ•°æ®å¤±è´¥ã€‚")
            print("ğŸ’¡ æŠ±æ­‰, 16GB VRAM æ— æ³•å¤„ç† r=32 + 20k æ•°æ®ã€‚è¯·åšæŒ V10 (10k æ•°æ®)ã€‚")
        else:
            raise e
        return

    # === ä¿å­˜ ===
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    try:
        model.save(output_path)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    except:
        lora_path = os.path.join(output_path, "lora_adapter")
        model[0].auto_model.save_pretrained(lora_path)
        print(f"âœ… å®Œæ•´ä¿å­˜å¤±è´¥, ä»… LoRA é€‚é…å™¨å·²ä¿å­˜åˆ°: {lora_path}")

    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š Qwen3-4B (5080 V13) æ€§èƒ½æ€»ç»“:")
    print(f"  - V10 (r=32, 10k) å³°å€¼: 65.0%")
    print(f"  - V13 (r=32, 20k) å¾®è°ƒå: æŸ¥çœ‹ä¸Šæ–¹æœ€ä½³å‡†ç¡®ç‡ (ç›®æ ‡ > 65.0%)")


if __name__ == "__main__":
    main()