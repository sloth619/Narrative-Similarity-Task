import json
import csv
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from tqdm import tqdm
import torch
from datetime import datetime
import time

from unsloth import FastLanguageModel

class Qwen3DataAugmentorOptimized:
    """V3.4 æœ€ç»ˆä¿®å¤ç‰ˆ (è‹±æ–‡æç¤ºè¯ + æ­£ç¡®é‡‡æ ·)"""

    def __init__(
        self,
        model_name: str = "/root/autodl-tmp/Qwen3-4B-Instruct-2507",
        max_seq_length: int = 2048,
        load_in_4bit: bool = True,  # é»˜è®¤ä½¿ç”¨ 4-bit
        dtype=None,
        device: str = "auto",
        checkpoint_dir: str = "./checkpoints"
    ):
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} (4-bit: {load_in_4bit})")

        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=max_seq_length,
            load_in_4bit=load_in_4bit,
            dtype=dtype,
        )

        FastLanguageModel.for_inference(self.model)

        # âœ… ä¿®å¤ 1: è®¾ç½® pad token å’Œ padding_side
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # âœ… å…³é”®ä¿®å¤ï¼šè®¾ç½®å·¦å¡«å……
        self.tokenizer.padding_side = 'left'

        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # <<< [FIXED] æç¤ºè¯å·²å…¨éƒ¨æ”¹ä¸ºè‹±æ–‡ >>>

        # ç¼“å­˜ç³»ç»Ÿæ¶ˆæ¯ (æ”¹ä¸ºè‹±æ–‡)
        self.system_message = {"role": "system", "content": "You are a professional data augmentation assistant, skilled at creating similar but not identical text samples."}

        # ç­–ç•¥1ï¼šç”Ÿæˆâ€œè´Ÿæ ·æœ¬â€ (æ”¹ä¸ºè‹±æ–‡)
        self.track_a_negative_gen_template = """You are an expert story reviewer. Your task is to create a "negative sample".

Please refer to the following two similar stories:

Anchor Story:
{anchor}

Positive Story (more similar to the Anchor Story):
{positive}

Requirement: {instruction}

Please create a **new negative story**. This new story should:
1. Be thematically related to the "Anchor Story".
2. Be clearly **less** similar to the "Anchor Story" than the "Positive Story" is.
3. Be completely different from the "Positive Story".

Please output **only the text content** of your new negative story, without any other explanatory text or JSON."""

        # ç­–ç•¥2ï¼šç”Ÿæˆâ€œæ­£æ ·æœ¬â€ (æ”¹ä¸ºè‹±æ–‡)
        self.track_a_positive_gen_template = """You are an expert story reviewer. Your task is to create a "positive sample".

Please refer to the following two stories:

Anchor Story:
{anchor}

Negative Story (not very similar to the Anchor Story):
{negative}

Requirement: {instruction}

Please create a **new positive story**. This new story should:
1. Be **highly similar** in plot and theme to the "Anchor Story".
2. Be clearly **more** similar to the "Anchor Story" than the "Negative Story" is.
3. Be completely different from the "Negative Story".

Please output **only the text content** of your new positive story, without any other explanatory text or JSON."""

        # Track B æç¤ºè¯ (æ”¹ä¸ºè‹±æ–‡)
        self.track_b_template = """You are an expert story reviewer. Please create a new, similar story based on the following story.

Original Story:
{text}

Requirement: {instruction}

Please output **only the text content** of the new story, without any other explanatory text."""

        # å¤šæ ·æ€§æŒ‡ä»¤ (æ”¹ä¸ºè‹±æ–‡)
        self.diversity_instructions = [
            "Maintain a similar theme and structure, but use different details and wording.",
            "Keep the core plot, but change the story's setting, time period, or cultural context.",
            "Maintain the story's theme, but retell it from a completely different angle or character's perspective."
        ]
        # <<< è‹±æ–‡æç¤ºè¯ä¿®å¤ç»“æŸ >>>

        print("æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"Padding side: {self.tokenizer.padding_side}") # éªŒè¯ Padding ä¿®å¤

    def load_jsonl(self, file_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½ JSONLines æ–‡ä»¶"""
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        return data

    def save_checkpoint(self, checkpoint_path: Union[Path, str], data: List[Dict[str, Any]],
                           current_index: int, current_round: int, metadata: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'data': data,
            'current_index': current_index,
            'current_round': current_round,
            'metadata': metadata,
            'timestamp': datetime.now().isoformat()
        }
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)

    def load_checkpoint(self, checkpoint_path: Union[Path, str]) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            return None
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)
            print(f"âœ“ å·²åŠ è½½æ£€æŸ¥ç‚¹: å·²æœ‰ {len(checkpoint['data'])} ä¸ªæ ·æœ¬")
            return checkpoint
        except Exception as e:
            print(f"âš  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

    def create_prompt_track_b(self, item: Dict[str, Any], diversity_level: int = 0) -> str:
        """å¿«é€Ÿåˆ›å»º Track B æç¤ºè¯"""
        return self.track_b_template.format(
            text=item.get('text', ''),
            instruction=self.diversity_instructions[min(diversity_level, 2)]
        )

    # <<< [FIXED] ä¿®å¤é‡‡æ ·å‚æ•° >>>
    def generate_text_batch(
        self,
        prompts: List[str],
        max_new_tokens: int = 512,
        # ä¿®æ­£ä¸º Qwen3-4B å®˜æ–¹æ¨èå‚æ•°
        temperature: float = 0.7, # ä» 0.8 é™ä¸º 0.7
        top_p: float = 0.8,       # ä» 0.9 é™ä¸º 0.8
        top_k: int = 20,        # ä» 50 é™ä¸º 20
        # å¢åŠ é‡å¤æƒ©ç½š
        repetition_penalty: float = 1.1
    ) -> List[str]:
        """ä¼˜åŒ–çš„æ‰¹å¤„ç†ç”Ÿæˆ (å·²ä¿®å¤é‡å¤é—®é¢˜)"""

        # 1. æ‰¹é‡å‡†å¤‡èŠå¤©æ¨¡æ¿
        messages_batch = [
            [self.system_message, {"role": "user", "content": p}]
            for p in prompts
        ]

        # 2. è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
        texts = [
            self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            for messages in messages_batch
        ]

        # 3. æ‰¹é‡ tokenizeï¼ˆå·¦å¡«å……ï¼‰
        inputs = self.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.model.config.max_position_embeddings,
        ).to(self.model.device)

        # 4. æ‰¹é‡ç”Ÿæˆ
        with torch.inference_mode():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty, # åº”ç”¨é‡å¤æƒ©ç½š
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,
            )

        # 5. æ‰¹é‡è§£ç 
        input_ids_len = inputs.input_ids.shape[1]
        generated_tokens = outputs[:, input_ids_len:]
        generated_texts = self.tokenizer.batch_decode(
            generated_tokens,
            skip_special_tokens=True
        )

        return [text.strip() for text in generated_texts]
    # <<< é‡‡æ ·å‚æ•°ä¿®å¤ç»“æŸ >>>

    def augment_track_a_batch(
        self,
        data: List[Dict[str, Any]],
        batch_size: int = 8,  # é»˜è®¤å°æ‰¹å¤„ç†
        target_total: int = 10000,
        checkpoint_interval: int = 500,
        checkpoint_name: str = "track_a_checkpoint.json",
        resume: bool = True,
        include_original: bool = True
    ) -> List[Dict[str, Any]]:
        """Track A æ•°æ®å¢å¼º - æ‰¹å¤„ç†ä¼˜åŒ–"""
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        checkpoint = None
        if resume:
            checkpoint = self.load_checkpoint(checkpoint_path)

        if checkpoint:
            augmented_data = checkpoint['data']
            start_index = checkpoint['current_index']
            start_round = checkpoint['current_round']
            metadata = checkpoint['metadata']
        else:
            augmented_data = []
            start_index = 0
            start_round = 0

            if include_original:
                augmented_data.extend(data)
                print(f"å·²æ·»åŠ  {len(data)} ä¸ªåŸå§‹æ ·æœ¬")

            original_count = len(data) if include_original else 0
            needed_augmentations = target_total - original_count
            if len(data) == 0:
                print("âš  è­¦å‘Š: åŸå§‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¢å¼ºã€‚")
                return []
            augmentations_per_sample = needed_augmentations // len(data) if len(data) > 0 else 0
            remainder = needed_augmentations % len(data) if len(data) > 0 else 0
            metadata = {
                'target_total': target_total,
                'original_count': len(data),
                'augmentations_per_sample': augmentations_per_sample,
                'remainder': remainder,
                'include_original': include_original
            }
            print(f"\n=== æ•°æ®å¢å¼ºè®¡åˆ’ ===")
            print(f"ç›®æ ‡æ ·æœ¬æ•°: {target_total}")
            print(f"éœ€è¦ç”Ÿæˆ: {needed_augmentations} ä¸ª")
            print(f"æ¯ä¸ªæ ·æœ¬ç”Ÿæˆ: {augmentations_per_sample} ä¸ª")
            print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")

        augmentations_per_sample = metadata['augmentations_per_sample']
        remainder = metadata['remainder']

        generation_tasks = []
        for idx, item in enumerate(data):
            if idx < start_index:
                continue
            current_augmentations = augmentations_per_sample
            if idx < remainder:
                current_augmentations += 1
            start_aug = start_round if idx == start_index else 0
            for aug_round in range(start_aug, current_augmentations):
                if len(augmented_data) + len(generation_tasks) >= target_total:
                    break
                diversity = aug_round % 3
                generation_tasks.append({
                    'item': item,
                    'source_index': idx,
                    'aug_round': aug_round,
                    'diversity': diversity
                })
            if len(augmented_data) + len(generation_tasks) >= target_total:
                break

        print(f"\nå‡†å¤‡ç”Ÿæˆ {len(generation_tasks)} ä¸ªæ ·æœ¬")

        samples_since_checkpoint = 0
        start_time = time.time()
        success_count = 0

        current_task_for_checkpoint = generation_tasks[0] if generation_tasks else {}

        try:
            for batch_start in tqdm(range(0, len(generation_tasks), batch_size), desc="ç”Ÿæˆè¿›åº¦"):
                batch_tasks = generation_tasks[batch_start : batch_start + batch_size]
                batch_prompts = []

                for task in batch_tasks:
                    item = task['item']
                    original_label_is_true = item.get('text_a_is_closer', True)
                    anchor = item.get('anchor_text', '')

                    gen_strategy = 'positive' if task['aug_round'] % 2 == 0 else 'negative'

                    if gen_strategy == 'positive':
                        negative_story = item.get('text_b', '') if original_label_is_true else item.get('text_a', '')
                        if not anchor or not negative_story:
                            batch_prompts.append(None)
                            continue
                        prompt = self.track_a_positive_gen_template.format(
                            anchor=anchor,
                            negative=negative_story,
                            instruction=self.diversity_instructions[task['diversity']]
                        )
                        batch_prompts.append(prompt)
                    else:
                        positive_story = item.get('text_a', '') if original_label_is_true else item.get('text_b', '')
                        if not anchor or not positive_story:
                            batch_prompts.append(None)
                            continue
                        prompt = self.track_a_negative_gen_template.format(
                            anchor=anchor,
                            positive=positive_story,
                            instruction=self.diversity_instructions[task['diversity']]
                        )
                        batch_prompts.append(prompt)

                valid_tasks_and_prompts = [
                    (task, prompt) for task, prompt in zip(batch_tasks, batch_prompts) if prompt is not None
                ]
                if not valid_tasks_and_prompts:
                    continue

                valid_tasks, valid_prompts = zip(*valid_tasks_and_prompts)

                generated_texts = self.generate_text_batch(
                    list(valid_prompts),
                    max_new_tokens=512
                )

                for j, generated_text in enumerate(generated_texts):
                    task = valid_tasks[j]
                    item = task['item']
                    original_label_is_true = item.get('text_a_is_closer', True)
                    anchor = item.get('anchor_text', '')
                    gen_strategy = 'positive' if task['aug_round'] % 2 == 0 else 'negative'

                    if not generated_text:
                        continue

                    try:
                        new_item = {"anchor_text": anchor}
                        gen_type_meta = "unknown"

                        if gen_strategy == 'positive':
                            negative_story = item.get('text_b', '') if original_label_is_true else item.get('text_a', '')
                            if original_label_is_true:
                                new_item["text_a"] = generated_text
                                new_item["text_b"] = negative_story
                                new_item["text_a_is_closer"] = True
                            else:
                                new_item["text_a"] = negative_story
                                new_item["text_b"] = generated_text
                                new_item["text_a_is_closer"] = False
                            gen_type_meta = "positive"
                        else:
                            positive_story = item.get('text_a', '') if original_label_is_true else item.get('text_b', '')
                            if original_label_is_true:
                                new_item["text_a"] = positive_story
                                new_item["text_b"] = generated_text
                                new_item["text_a_is_closer"] = True
                            else:
                                new_item["text_a"] = generated_text
                                new_item["text_b"] = positive_story
                                new_item["text_a_is_closer"] = False
                            gen_type_meta = "negative"

                        new_item['_augmented'] = True
                        new_item['_source_index'] = task['source_index']
                        new_item['_augmentation_round'] = task['aug_round'] + 1
                        new_item['_diversity_level'] = task['diversity']
                        new_item['_gen_strategy'] = gen_type_meta

                        augmented_data.append(new_item)
                        success_count += 1

                    except Exception as e:
                        print(f"\nâš  è­¦å‘Š: å¤„ç†ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue

                samples_since_checkpoint += len(batch_tasks)
                current_task_for_checkpoint = batch_tasks[0]

                if samples_since_checkpoint >= checkpoint_interval:
                    elapsed = time.time() - start_time
                    speed = success_count / elapsed if elapsed > 0 else 0
                    print(f"\n[æ£€æŸ¥ç‚¹] å·²ç”Ÿæˆ {success_count} ä¸ªæ ·æœ¬, é€Ÿåº¦: {speed:.2f} æ ·æœ¬/ç§’")

                    self.save_checkpoint(
                        checkpoint_path,
                        augmented_data,
                        current_task_for_checkpoint['source_index'],
                        current_task_for_checkpoint['aug_round'] + 1,
                        metadata
                    )
                    samples_since_checkpoint = 0

                if len(augmented_data) >= target_total:
                    break

        except KeyboardInterrupt:
            print("\nâš  ä¸­æ–­ï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
            self.save_checkpoint(
                checkpoint_path,
                augmented_data,
                current_task_for_checkpoint.get('source_index', 0),
                current_task_for_checkpoint.get('aug_round', 0),
                metadata
            )
            return augmented_data

        elapsed = time.time() - start_time
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼")
        if elapsed > 0:
            print(f"   æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
            print(f"   å¹³å‡é€Ÿåº¦: {success_count/elapsed:.2f} æ ·æœ¬/ç§’")

        self.save_checkpoint(checkpoint_path, augmented_data, len(data), 0, metadata)
        return augmented_data[:target_total]

    def augment_track_b_batch(
        self,
        data: List[Dict[str, Any]],
        batch_size: int = 8,
        target_total: int = 10000,
        checkpoint_interval: int = 500,
        checkpoint_name: str = "track_b_checkpoint.json",
        resume: bool = True,
        include_original: bool = True
    ) -> List[Dict[str, Any]]:
        """Track B æ•°æ®å¢å¼º - æ‰¹å¤„ç†ä¼˜åŒ–"""
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        checkpoint = None
        if resume:
            checkpoint = self.load_checkpoint(checkpoint_path)
        if checkpoint:
            augmented_data = checkpoint['data']
            start_index = checkpoint['current_index']
            start_round = checkpoint['current_round']
            metadata = checkpoint['metadata']
        else:
            augmented_data = []
            start_index = 0
            start_round = 0
            if include_original:
                augmented_data.extend(data)
            original_count = len(data) if include_original else 0
            needed_augmentations = target_total - original_count
            if len(data) == 0:
                print("âš  è­¦å‘Š: åŸå§‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¢å¼ºã€‚")
                return []
            augmentations_per_sample = needed_augmentations // len(data) if len(data) > 0 else 0
            remainder = needed_augmentations % len(data) if len(data) > 0 else 0
            metadata = {
                'target_total': target_total,
                'original_count': len(data),
                'augmentations_per_sample': augmentations_per_sample,
                'remainder': remainder,
                'include_original': include_original
            }

        augmentations_per_sample = metadata['augmentations_per_sample']
        remainder = metadata['remainder']

        generation_tasks = []
        for idx, item in enumerate(data):
            if idx < start_index:
                continue
            current_augmentations = augmentations_per_sample
            if idx < remainder:
                current_augmentations += 1
            start_aug = start_round if idx == start_index else 0
            for aug_round in range(start_aug, current_augmentations):
                if len(augmented_data) + len(generation_tasks) >= target_total:
                    break
                diversity = aug_round % 3
                generation_tasks.append({
                    'item': item,
                    'source_index': idx,
                    'aug_round': aug_round,
                    'diversity': diversity
                })
            if len(augmented_data) + len(generation_tasks) >= target_total:
                break

        samples_since_checkpoint = 0
        start_time = time.time()
        success_count = 0

        current_task_for_checkpoint = generation_tasks[0] if generation_tasks else {}

        try:
            for batch_start in tqdm(range(0, len(generation_tasks), batch_size), desc="ç”Ÿæˆè¿›åº¦"):
                batch_tasks = generation_tasks[batch_start : batch_start + batch_size]

                batch_prompts = [
                    self.create_prompt_track_b(task['item'], task['diversity'])
                    for task in batch_tasks
                ]

                generated_texts = self.generate_text_batch(
                    batch_prompts,
                    max_new_tokens=512
                )

                for j, generated_text in enumerate(generated_texts):
                    task = batch_tasks[j]

                    if not generated_text:
                        continue

                    try:
                        new_item = {
                            'text': generated_text,
                            '_augmented': True,
                            '_source_index': task['source_index'],
                            '_augmentation_round': task['aug_round'] + 1,
                            '_diversity_level': task['diversity']
                        }
                        augmented_data.append(new_item)
                        success_count += 1
                    except Exception as e:
                        print(f"\nâš  è­¦å‘Š: å¤„ç†ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue

                samples_since_checkpoint += len(batch_tasks)
                current_task_for_checkpoint = batch_tasks[0]

                if samples_since_checkpoint >= checkpoint_interval:
                    elapsed = time.time() - start_time
                    speed = success_count / elapsed if elapsed > 0 else 0
                    print(f"\n[æ£€æŸ¥ç‚¹] å·²ç”Ÿæˆ {success_count} ä¸ªæ ·æœ¬, é€Ÿåº¦: {speed:.2f} æ ·æœ¬/ç§’")

                    self.save_checkpoint(
                        checkpoint_path,
                        augmented_data,
                        current_task_for_checkpoint['source_index'],
                        current_task_for_checkpoint['aug_round'] + 1,
                        metadata
                    )
                    samples_since_checkpoint = 0

                if len(augmented_data) >= target_total:
                    break

        except KeyboardInterrupt:
            print("\nâš  ä¸­æ–­ï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
            self.save_checkpoint(
                checkpoint_path,
                augmented_data,
                current_task_for_checkpoint.get('source_index', 0),
                current_task_for_checkpoint.get('aug_round', 0),
                metadata
            )
            return augmented_data

        elapsed = time.time() - start_time
        if elapsed > 0:
            print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")

        self.save_checkpoint(checkpoint_path, augmented_data, len(data), 0, metadata)
        return augmented_data[:target_total]

    def save_to_jsonl(self, data: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜ä¸º JSONLines æ ¼å¼"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"âœ“ å·²ä¿å­˜åˆ°: {output_path}")

    def save_to_csv(self, data: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜ä¸º CSV æ ¼å¼"""
        if not data:
            print("âš  è­¦å‘Š: æ²¡æœ‰æ•°æ®å¯ä¿å­˜åˆ° CSVã€‚")
            return

        all_keys = set()
        for item in data:
            if isinstance(item, dict):
                all_keys.update(item.keys())
        fieldnames = sorted(list(all_keys))
        if not fieldnames:
            print("âš  è­¦å‘Š: æœªèƒ½ä»æ•°æ®ä¸­æå–ä»»ä½• CSV å­—æ®µã€‚")
            return

        try:
            with open(output_path, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
                writer.writeheader()
                dict_data = [item for item in data if isinstance(item, dict)]
                writer.writerows(dict_data)
            print(f"âœ“ å·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ CSV å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import os
    os.environ['TRANSFORMERS_OFFLINE'] = '1'
    os.environ['HF_HUB_OFFLINE'] = '1'

    # âœ… ä½¿ç”¨ 4-bit é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜
    augmentor = Qwen3DataAugmentorOptimized(
        model_name="/root/autodl-tmp/Qwen3-4B-Instruct-2507",
        max_seq_length=2048,
        load_in_4bit=True,     # 4-bit é‡åŒ–
        dtype=None,
        checkpoint_dir="./checkpoints"
    )

    print("\n" + "="*70)
    print("ğŸš€ æ··åˆæ•°æ®å¢å¼º (V3.4 æœ€ç»ˆä¿®å¤ç‰ˆ - 4B @ 4-bit)")
    print("="*70)

    # âœ… ä½¿ç”¨ä¿å®ˆçš„æ‰¹å¤„ç†å¤§å°
    EFFECTIVE_BATCH_SIZE = 100  # å¯ä»¥æ ¹æ®å®é™…æ˜¾å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´

    print("\nğŸ“Š Track A: åŠ è½½æ•°æ®")
    dev_a = augmentor.load_jsonl("../TrainData/dev_track_a.jsonl")
    synthetic_a = augmentor.load_jsonl("../TrainData/synthetic_data_for_classification.jsonl")
    mixed_a_data = dev_a + synthetic_a
    print(f"åŸºç¡€æ ·æœ¬: {len(mixed_a_data)} ä¸ª")

    print("\nğŸ”„ å¼€å§‹ç”Ÿæˆ Track A...")
    augmented_mixed_a = augmentor.augment_track_a_batch(
        mixed_a_data,
        batch_size=EFFECTIVE_BATCH_SIZE,
        target_total=10000,
        checkpoint_interval=500,
        checkpoint_name="track_a_mixed_10k_opt.json",
        resume=True,
        include_original=True
    )

    print(f"\nâœ… Track A å®Œæˆ: {len(augmented_mixed_a)} ä¸ªæ ·æœ¬")
    augmentor.save_to_jsonl(augmented_mixed_a, "train_track_a_mixed_10k.jsonl")
    augmentor.save_to_csv(augmented_mixed_a, "train_track_a_mixed_10k.csv")

    print("\nğŸ“Š Track B: åŠ è½½æ•°æ®")
    dev_b = augmentor.load_jsonl("../TrainData/dev_track_b.jsonl")
    synthetic_b = augmentor.load_jsonl("../TrainData/synthetic_data_for_contrastive_learning.jsonl")
    mixed_b_data = dev_b + synthetic_b
    print(f"åŸºç¡€æ ·æœ¬: {len(mixed_b_data)} ä¸ª")

    print("\nğŸ”„ å¼€å§‹ç”Ÿæˆ Track B...")
    augmented_mixed_b = augmentor.augment_track_b_batch(
        mixed_b_data,
        batch_size=EFFECTIVE_BATCH_SIZE,
        target_total=10000,
        checkpoint_interval=500,
        checkpoint_name="track_b_mixed_10k_opt.json",
        resume=True,
        include_original=True
    )

    print(f"\nâœ… Track B å®Œæˆ: {len(augmented_mixed_b)} ä¸ªæ ·æœ¬")
    augmentor.save_to_jsonl(augmented_mixed_b, "train_track_b_mixed_10k.jsonl")
    augmentor.save_to_csv(augmented_mixed_b, "train_track_b_mixed_10k.csv")

    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()